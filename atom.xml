<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>xmgooo</title>
  
  <subtitle>take it easy</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-28T16:19:53.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>xiemao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>redis总结</title>
    <link href="http://yoursite.com/2018/11/24/redis%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/11/24/redis总结/</id>
    <published>2018-11-24T04:43:29.000Z</published>
    <updated>2018-11-28T16:19:53.000Z</updated>
    
    <content type="html"><![CDATA[<h1>关于redis的一点总结</h1><hr><h2>引言</h2><p>     常规的应用系统的系统瓶颈大部分都出现在数据库的IO操作，并且90%的请求基本都是select操作，对于数据库的优化手段可以有非常多的手段，举几种常见的:&lt;!--more--&gt;1.最基本的数据库表结构设计优化，理论上的三大范式，表结构结合业务设计的合理性，是否可从业务角度优化，如两次查询变为一次查询，联表查变为单表操作，跨库操作是否可以避免或者优化，sql语句的优化等。2.索引，代价最小，实现难度最低，效果提升显著，但要考虑索引维护带来的性能开销，对于一些批量更新或者集中的update，delete操作，可以考虑动态索引，即在开启操作前删除索引，执行完后再添加上索引。3.mysql参数的优化，连接数，缓冲池大小等。4.读写分离，扩展slave节点，分担读请求的压力，但要注意数据同步操作可能带来的延迟性问题。5.大表拆分，这里可分为水平和垂直两个方向。水平拆分，分表，减少单表数据量过大的问题，一般mysql单表数据控制在1000w性能最优，注意拆分业务的合理性，常用的可以有主键hash，业务字段取余（如日期，订单），但要提前规划好容量。垂直拆分，1次查询变为两次关联，冷热数据字段分离等。6.分库，应对单库性能瓶颈的一种手段，但要注意可能带来的跨库操作，或者复杂查询（count，group by，top）等增加的复杂性。7.缓存。</p><h2>缓存</h2><p>     关于缓存，缓存分为很多种，页面缓存cookie，基于会话，session的临时缓存springcache，hibernate，mybatis等，基于内存的redis/memcached等缓存，常用的redis和memcached，memcached没研究过，不过多说了，网上对比的资料也很多，感觉能用memcached的地方都能用redis代替，并且memcached不支持持久化操作和事务，这点限定了它的很多使用场景，至于它的在小数据量下相比于redis的性能优点，redis通过集群的手段也能有所弥补，本身已经够快了，那点速度的差别也微不足道了。在请求量特别大的时候，可以考虑三级缓存，一级是nginx的本地缓存，在网关这一层做处理，二级缓存通过应用服务器的堆内存，基于进程，不过这种缓存是存在某一台应用服务器的（当然也可以同步，没必要），可能需要将ng等负载均衡服务器的策略改为定向转发而不是轮询等，三级缓存再考虑redis集群等。</p><h2>redis特点</h2><p>1.非关系型数据库（数据模型简单）2.基于内存，基于内存优于基于磁盘本身的特色(ps个笑话：因为内存距离cpu的距离比硬盘更近，所以性能好:) ,读写能力高。具体还要考虑业务的复杂性，如开启aof的always的话QPS的话性能可能会有影响，everysec这种的话QPS单机轻松上万，合理配置读写分离十万+也没问题的，搭配集群做分区分散请求配合缓冲队列，应对上百万，上千万的QPS也是能够实现的，具体的性能报告可疑查看官网。3.单线程，和多线程相比的优势，避免的线程的上下文切换和锁竞争问题，加锁释放锁等资源消耗，死锁等一系列问题都可以被屏蔽掉，单线程的特色决定了cpu本身就不可能成为性能瓶颈，但是在应对多核cpu的处理器，单线程就可能显的比较鸡肋和奢侈了，这种的应对办法一般是在单机上部署多个redis实例，比如8核的cpu，理论上可以部署8个redis实例，但是这也只是想象的，因为本身redis的持久化操作和数据同步等操作都会开启临时子线程去处理，如果部署太多实例，实例间的通信和同步带来的开销可能反而会使性能降低，所以需综合cpu的实际情况进行考量，真正决定redis性能的在于实际内存大小和网络io两方面。网络io的话我理解的优化方向批量提交，较少io次数，序列化请求数据，protobuf，kyro等来减少io传输的数据字节。关于内存的使用,建议不要让你的 Redis 所在机器物理内存使用超过实际内存总量的3/5，内存一般控制在10GB以内，内存太大aof的rewrite基于当前的快照等会影响到主进程的执行，详细分析可以参考下https://mp.weixin.qq.com/s/5TtTplThal2otR8xJB6OEw （这个有待测试验证）</p><h2>持久化操作：rdb和aof</h2><p>     redis持久化的意义，在于故障恢复，平时可能感受不到持久化机制的作用，一旦redis发生故障，如果没有持久化的话，就会丢失所有的数据，如果通过持久化机制进行备份并定期同步到云存储服务上去，可以最大程度的保证发生故障时数据不丢失并快速提供正常的服务<br>     rbd：周期性的刷盘，相当于linux中的快照概念，所以也被称为快照持久化，如每一秒定时将内存中的数据写入硬盘中，不适用于实时的系统，如想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。save命令和bgsave命令都可以生成RDB文件，线上环境应该禁掉save命令，这是个同步操作，会阻塞主进程，而bgsave命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。     aof：类似于mysql的binlog，和oracle中的undo，实时将每次执行的写命令保存到硬盘，宕机或者掉电后数据可恢复，但实时的日志会带来额外的性能开销。<br>     两种方式的取舍主要是在持久化的性能和一致性之间做取舍。建议生产环境开启aof配置成everysec即可<br>给出自己的一点理解吧：1.晚上执行定时任务跑批量或者缓存更新备份或者数据恢复的时候会用rdb，跑批失败了可以恢复到跑批前的快照，重新执行定时任务或者重新备份即可。2.并发量不大情况下，对数据完整性要求较高时，可启用aof的always。3.若只打算用Redis 做缓存，可以关闭持久化，仅靠master／salve replication实现高可用也行，能节省一大笔io,代价是如果master／salve同时宕机（此时建议采用交叉部署，master部署在a机房，slave1部署在b机房，slave2部署在c机房，虽然跨机房的部署同步会增加io，但相比于aof的io就显得微不足道了，这种可以将一个集群同时宕机的风险降到最低，只要一个节点活着，数据即是完整，除非你说北京，上海，广州的机房同时炸了。）4.rdb其实更多用在云存储的冷数据备份，除了在故障恢复时快一点以外，个人感觉还是要使用aof的方式5.关于是在master还是slave节点做持久化操作的问题：讲一个案例，数据持久化的操作一定要在master节点开启，虽然会造成master节点写入性能的不稳定，但是我碰到过一个问题，主从切换时，master短暂的宕机，然后自动重启，哨兵或者其它节点还没监测到master的故障，但在瞬间内存中的数据被清空，然后将空数据同步到下面的salve节点，导致百分之百的数据丢失，非常坑爹，对于这个问题两个建议：一是直接关闭redis的自动重启功能，等待心跳检测发现master宕机，切换，然后触发脚本去重启master，之后作为slave加入集群，走完常规的流程，重启的速度没有自动重启那么快，因为依赖于心跳检测去发现故障。二是直接在master做持久化操作。所以对于这个问题，建议还是要在master开启持久化操作。6.对于节点间的同步io消耗，如果将master和slave部署在同一台机器上，确实会极大的降低io，但是不建议这么做，还是建议采取交叉部署的方式，将同一个分区的master和slave分散在不同机器。fsync控制着缓冲区的写入刷盘时间，这个参数的配置直接影响着redis的QPS(always,everysec,no)</p><h2>版本比较</h2><p>     2.6版本推出了哨兵的机制，但是特别不稳定，各种各样的bug，2.8版本这个功能才算稳定下来，2.x哨兵的作用是一是监控主从，定时的心跳检测包看集群是否健康，二是实现切换，master宕机后通过选举算法在此master下的slave中找一个顶上来。可以配置多个master做高可用，master与master实现复制同步。即使使用哨兵，redis每个实例也是全量存储，每个redis存储的内容都是完整的数据，浪费内存，单点容量上限无法突破。这里比较知名的企业或者组织就衍生开源了一系列方案，为了做数据分片，大致两种思路，分为客户端分片和服务端分片，客户端分片通过代码在客户端实现，但是存在可扩展性差，不能跨语言，不通用等问题，服务端分片都是加一层proxy代理层集群加zk来做的，通过proxy代理层重定向到各个集群分片，实现难度较大，增加了系统复杂度（一个很坑和很冷的笑话：没有什么架构问题是向上加一层不能解决的，如果不能，那就加两层）     3.x版本较2.x版本的master/slave+哨兵的架构上有了很大扩展，抛弃了之前的架构，基于分片进行存储，采用hash进行分区映射，解决了以前master/slave+哨兵模式中的内存/QPS受限于单机的困境，即每台master redis存储不同的内容，master节点上具备slot槽的概念，存储写入时通过hash确定写入到哪个节点，读取时也是先确定数据在哪个槽上，再找到对应的节点，之后在本机上进行读取，这里要说明下，每个master节点下可挂载多个slave节点，slave节点本身是不具备槽的。有时为了解决master下挂载过多slave数据同步带来的延迟问题，可能会采用master下只挂在一个slave1，slave1下再挂在slave2，slave2下再挂在slave3这种串行的架构，这种模式其实我没明白，如果slave1断开的话，后面的一串跟master怎么通信（有待考核）     集群的这种架构去中心化，内联通信，本身master节点又可以存储数据，又可以与其它master进行通信，可用于大数据量高可用Cache/存储等场景。这种内联的架构感觉更符合现代互联网的架构模式，现代大数据框架都是采用这种形式，像hadoop，storm等，将监控与应用结合在一起，每个节点即是执行者也是监督者，节点之间互相通信，整个流程跟哨兵相比，非常类似，所以说，redis cluster功能强大，直接集成了replication和sentinal的功能，底层是采用gossip协议进行通信，互相之间不断通信，保持整个集群所有节点的数据信息是完整的。客户端的请求会有一个重定向的过程，客户端可能会挑选任意一个redis实例去发送命令，每个redis实例接收到命令，都会计算key对应的hash slot如果在本地就在本地处理，否则返回moved给客户端，让客户端进行重定向cluster keyslot mykey，可以查看一个key对应的hash slot是什么，用redis-cli的时候，可以加入-c参数，支持自动的请求重定向，redis-cli接收到moved之后，会自动重定向到对应的节点执行命令</p><h2>主从复制</h2><p>     master到slave的复制是异步进行的，分两种:一种是全量复制，全量复制复制的是rdb文件，也就是真实的数据文件，这种发生在新加入slave节点扩充集群时，slave节点发送一个psync的命令给master，此时master生成一份当前时间的rdb快照，并将数据发送给slave，slave先写入到磁盘，再将磁盘的数据读取到内存中，这个过程完成前slave是不对外提供服务的。一种是增量复制，增量复制复制的是增量的命令文件，分为数据同步和数据恢复，数据同步是写入master的数据返回客户端后会异步将命令发给salve节点，salve本地执行命令后加载进内存后再进行持久化操作，所以这种是先加载进内存，也是为了保证从节点尽快的做到数据更新一致，slave节点也会定时的去检测master数据的变化，数据恢复是在slave宕机的这段时间，master仅仅将slave节点缺失的数据发送给slave。<br>     2.8版本在主从复制这里做了断点续传的优化，举个例子，一个master上新加入了一个slave节点，此时slave节点在做全量复制的过程中如果宕机了，此时会打上标记，重启后从断开的位置继续进行复制。<br>     从节点永远不会主动删除数据，这也是为了保证数据一致性。对于master缓存数据的失效，如LRU淘汰一个key或者缓存过期，salve本身是没有这种机制的，都是master模拟发送一条del命令到了slave节点。</p><h2>主从切换</h2><p>     先直接说主从切换过程中可能带来的数据丢失问题，两种场景：1.异步复制导致的数据丢失问题，因为master到slave的复制是异步的，所以可能有部分数据还没复制到slave，master就宕机了，此时这些部分数据就丢失了。2.脑裂，也就是说，某个master所在机器突然脱离了正常的网络，跟其他slave机器不能连接，但是实际上master还运行着，此时哨兵可能就会认为master宕机了，然后开启选举，将其他slave切换成了master，这个时候，集群里就会有两个master，也就是所谓的脑裂，此时虽然某个slave被切换成了master，但是可能client还没来得及切换到新的master，还继续写向旧master的数据可能也丢失了，因此旧master再次恢复的时候，会被作为一个slave挂到新的master上去，自己的数据会清空，重新从新的master复制数据。对于这两个问题，首先一点更改下这两个配置<br>min-slaves-to-write 1<br>min-slaves-max-lag 10<br>要求至少有1个slave，数据复制和同步的延迟不能超过10秒，如果说一旦所有的slave，数据复制和同步的延迟都超过了10秒钟，那么这个时候，master就不会再接收任何请求了，上面两个配置可以减少异步复制和脑裂导致的数据丢失<br>     对于减少异步复制的数据丢失，有了min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就拒绝写请求（客户端降级，数据先灌入kafka），这样可以把master宕机时由于部分数据未同步到slave导致的数据丢失降低的可控范围内<br>     对于减少脑裂的数据丢失，如果一个master出现了脑裂，跟其他slave丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的slave发送数据，而且slave超过10秒没有给自己ack消息，那么就直接拒绝客户端的写请求，这样脑裂后的旧master就不会接受client的新数据，也就避免了数据丢失，上面的配置就确保了，如果跟任何一个slave丢了连接，在10秒后发现没有slave给自己ack，那么就拒绝新的写请求，因此在脑裂场景下，最多就丢失10秒的数据</p><h2>选举算法</h2><p>     先说下节点宕机的状态，sdown和odown两种失败状态，sdown是主观宕机，就一个哨兵如果自己觉得一个master宕机了，那么就是主观宕机，odown是客观宕机，如果quorum数量的哨兵都觉得一个master宕机了，那么就是客观宕机，sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-master-down-after-milliseconds指定的毫秒数之后，就主观认为master宕机，sdown到odown转换的条件很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其他哨兵也认为那个master是sdown了，那么就认为是odown了，客观认为master宕机。<br>     再说下哨兵集群的自动发现机制，哨兵互相之间的发现，是通过redis的pub/sub系统实现的，每个哨兵都会往__sentinel__:hello这个channel里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在，每隔两秒钟，每个哨兵都会往自己监控的某个master+slaves对应的__sentinel__:hello channel里发送一个消息，内容是自己的host、ip和runid还有对这个master的监控配置，每个哨兵也会去监听自己监控的每个master+slaves对应的__sentinel__:hello channel，然后去感知到同样在监听这个master+slaves的其他哨兵的存在，每个哨兵还会跟其他哨兵交换对master的监控配置，互相进行监控配置的同步。<br>     slave配置的自动纠正：哨兵会负责自动纠正slave的一些配置，比如slave如果要成为潜在的master候选人，哨兵会确保slave在复制现有master的数据; 如果slave连接到了一个错误的master上，比如故障转移之后，那么哨兵会确保它们连接到正确的master上 。     slave切换为master选举算法：如果一个master被认为odown了，而且majority哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个slave来，会考虑slave的一些信息：（1）跟master断开连接的时长。（2）slave优先级。（3）复制offset。（4）run id。如果一个slave跟master断开连接已经超过了down-after-milliseconds的10倍，外加master宕机的时长，那么slave就被认为不适合选举为master，(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state，接下来会对slave进行排序：（1）按照slave优先级进行排序，slave priority越低，优先级就越高。（2）如果slave priority相同，那么看replica offset，哪个slave复制了越多的数据，offset越靠后，优先级就越高。（3）如果上面两个条件都相同，那么选择一个run id比较小的那个slave。     quorum和majority，每次一个哨兵要做主备切换，首先需要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，这个哨兵还得得到majority哨兵的授权，才能正式执行切换，如果quorum &lt; majority，比如5个哨兵，majority就是3，quorum设置为2，那么就3个哨兵授权就可以执行切换，但是如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵，quorum是5，那么必须5个哨兵都同意授权，才能执行切换。<br>     configuration epoch：哨兵会对一套redis master+slave进行监控，有相应的监控的配置，执行切换的那个哨兵，会从要切换到的新master（salve-&gt;master）那里得到一个configuration epoch，这就是一个version号，每次切换的version号都必须是唯一的，如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待failover-timeout时间，然后接替继续执行切换，此时会重新获取一个新的configuration epoch，作为新的version号。<br>     configuraiton传播：哨兵完成切换之后，会在自己本地更新生成最新的master配置，然后同步给其他的哨兵，就是通过之前说的pub/sub消息机制。这里之前的version号就很重要了，因为各种消息都是通过一个channel去发布和监听的，所以一个哨兵完成一次新的切换之后，新的master配置是跟着新的version号的。其他的哨兵都是根据版本号的大小来更新自己的master配置的。</p><h2>架构演进图</h2><p><img src="/2018/11/24/redis总结/%E5%8D%95%E8%8A%82%E7%82%B9.jpg" alt="1.jpg">&lt;center&gt;单节点&lt;/center&gt;<img src="/2018/11/24/redis总结/slave%E8%8A%82%E7%82%B9%E5%88%86%E6%8B%85%E8%AF%BB%E5%8E%8B%E5%8A%9B.jpg" alt="1.jpg">&lt;center&gt;slave节点分担读压力&lt;/center&gt;<img src="/2018/11/24/redis总结/%E5%8A%A0%E5%85%A5%E5%93%A8%E5%85%B5.jpg" alt="1.jpg">&lt;center&gt;2.6版本后加入哨兵集群，实现主从切换&lt;/center&gt;<br><img src="/2018/11/24/redis总结/2x%E7%89%88%E6%9C%AC%E5%AE%9E%E7%8E%B0%E5%88%86%E7%89%87.jpg" alt="1.jpg">&lt;center&gt;2.x版本通过proxy实现分片存储，减少master写压力&lt;/center&gt;<br><img src="/2018/11/24/redis总结/3x%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4.jpg" alt="1.jpg">&lt;center&gt;3x版本集群&lt;/center&gt;</p><h2>单实例安装</h2><p>下载地址http://redis.io/download安装步骤：1 首先需要安装gcc，把下载好的redis-3.0.0-rc2.tar.gz 放到linux /usr/local文件夹下2 进行解压 tar -zxvf redis-3.0.0-rc2.tar.gz3 进入到redis-3.0.0目录下，进行编译 make4 进入到src下进行安装 make install  验证(ll查看src下的目录，有redis-server 、redis-cil即可)5 建立俩个文件夹存放redis命令和配置文件mkdir -p /usr/local/redis/etcmkdir -p /usr/local/redis/bin6 把redis-3.0.0下的redis.conf 移动到/usr/local/redis/etc下，<br>cp redis.conf /usr/local/redis/etc/7 把redis-3.0.0/src里的mkreleasehdr.sh、redis-benchmark、redis-check-aof、redis-check-dump、redis-cli、redis-server文件移动到bin下，命令：mv mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-dump redis-cli redis-server /usr/local/redis/bin8 启动时并指定配置文件：/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf（注意要使用后台启动，所以修改redis.conf里的 daemonize 改为yes)9 验证启动是否成功： ps -ef | grep redis 查看是否有redis服务 或者 查看端口：netstat -tunpl | grep 6379进入redis客户端 ./redis-cli 退出客户端quit退出redis服务：（1）pkill redis-server 、（2）kill 进程号、<br>（3）/usr/local/redis/bin/redis-cli shutdown</p><h2>哨兵模式的安装</h2><p>（1）copy文件sentinel.conf到usr/local/redis/etc中(2) 修改sentinel.conf文件:<br>sentinel monitor mymaster 192.168.1.174 6379 1 #名称，ip，端口，投票选举次数sentinel down-after-milliseconds mymaster 5000 #默认1s检测一次，这里配置超时5000毫秒为宕机sentinel failover-timeout mymaster 9000000sentinel can-failover mymaster yessentinel parallel-syncs mymaster 2(3) 启动sentinel哨兵/usr/local/redis/bin/redis-server /usr/local/redis/etc/sentinel.conf --sentinel &amp;(4) 查看哨兵相关信息命令/usr/local/redis/bin/redis-cli -h 192.168.1.175 -p 26379 info sentinel(5) 关闭主服务器/usr/local/redis/bin/redis-cli -h 192.168.1.175 -p 6379 shutdown</p><h2>Redis 3.x 集群搭建</h2><p>/********************* Redis 3.0 集群搭建 ********************************/在redis3.0以前，提供了Sentinel工具来监控各Master的状态;如果Master异常，则会做主从切换，将slave作为master，将master作为slave。其配置也是稍微的复杂，并且各方面表现一般。现在redis3.0已经支持集群的容错功能，并且非常简单。下面我们来进行学习下redis3.0如何搭建集群（集群搭建：至少要三个master）。第一步：创建一个文件夹redis-cluster，然后在其下面分别创建6个文件夹如下：（1）mkdir -p /usr/local/redis-cluster（2）mkdir 7001、mkdir 7002、mkdir 7003、mkdir 7004、mkdir 7005、mkdir 7006</p><p>第二步：把之前的redis.conf配置文件分别copy到700<em>下，进行修改各个文件内容，也就是对700</em>下的每一个copy的redis.conf文件进行修改！如下：（1）daemonize yes（2）port 700*（分别对每个机器的端口号进行设置）（3）bind 192.168.1.171（必须要绑定当前机器的ip，不然会无限悲剧下去哇..深坑勿入！！！）（4）dir /usr/local/redis-cluster/700*/（指定数据文件存放位置，必须要指定不同的目录位置，不然会丢失数据，深坑勿入！！！）（5）cluster-enabled yes（启动集群模式，开始玩耍）（6）cluster-config-file nodes700*.conf（这里700x最好和port对应上）（7）cluster-node-timeout 5000（8）append only yes数据文件dump.rdb放入到etc目录下</p><p>第三步：注意每个文件要修改端口号，bind的ip，数据存放的dir，并且nodes文件都需要进行修改！</p><p>第四步：由于redis集群需要使用ruby命令，所以我们需要安装ruby（1）yum install ruby（2）yum install rubygems（3）gem install redis （安装redis和ruby的接口）</p><p>第五步：分别启动6个redis实例，然后检查是否启动成功（1）/usr/local/redis/bin/redis-server /usr/local/redis-cluster/700*/redis.conf（2）ps -el | grep redis 查看是否启动成功</p><p>第六步：首先到redis3.0的安装目录下，然后执行redis-trib.rb命令。（1）cd /usr/local/redis3.0/src（2）./redis-trib.rb  create --replicas 1 10.211.55.6:7001 10.211.55.6:7002 10.211.55.6:7003 10.211.55.6:7004 10.211.55.6:7005 10.211.55.6:7006</p><p>第七步：到此为止我们集群搭建成功！进行验证：（1）连接任意一个客户端即可：./redis-cli -c -h -p （-c表示集群模式，指定ip地址和端口号）如：/usr/local/redis/bin/redis-cli -c -h 10.211.55.6 -p 7001（2）进行验证：cluster info（查看集群信息）、cluster nodes（查看节点列表）<img src="/2018/11/24/redis总结/clusterinfo.jpg" alt="1.jpg">上图集群节点信息说明一下，从左到右分别为&lt;id&gt; &lt;ip:port&gt; &lt;flags&gt; &lt;master&gt; &lt;ping-sent&gt; &lt;pong-recv&gt; &lt;config-epoch&gt; &lt;link-state&gt; &lt;slot&gt; &lt;slot&gt; ... &lt;slot&gt;1.id: 节点ID,是一个40字节的随机字符串，这个值在节点启动的时候创建，并且永远不会改变（除非使用CLUSTER RESET HARD命令）。2.ip:port: 客户端与节点通信使用的地址.3.flags: 逗号分割的标记位，可能的值有: myself, master, slave, fail?, fail, handshake, noaddr, noflags. 下一部分将详细介绍这些标记.4.master: 如果节点是slave，并且已知master节点，则这里列出master节点ID,否则的话这里列出”-“。5.ping-sent: 最近一次发送ping的时间，这个时间是一个unix毫秒时间戳，0代表没有发送过.6.pong-recv: 最近一次收到pong的时间，使用unix时间戳表示.7.config-epoch: 节点的epoch值（or of the current master if the node is a slave）。每当节点发生失败切换时，都会创建一个新的，独特的，递增的epoch。如果多个节点竞争同一个哈希槽时，epoch值更高的节点会抢夺到。8.link-state: node-to-node集群总线使用的链接的状态，我们使用这个链接与集群中其他节点进行通信.值可以是 connected 和 disconnected.9.slot: 哈希槽值或者一个哈希槽范围. 从第9个参数开始，后面最多可能有16384个 数(limit never reached)。代表当前节点可以提供服务的所有哈希槽值。如果只是一个值,那就是只有一个槽会被使用。如果是一个范围，这个值表示为起始槽-结束槽，节点将处理包括起始槽和结束槽在内的所有哈希槽。各flags的含义 (上面所说数据项3):myself: 当前连接的节点.master: 节点是master.slave: 节点是slave.fail?: 节点处于PFAIL 状态。 当前节点无法联系，但逻辑上是可达的 (非 FAIL 状态).fail: 节点处于FAIL 状态. 大部分节点都无法与其取得联系将会将改节点由 PFAIL 状态升级至FAIL状态。handshake: 还未取得信任的节点，当前正在与其进行握手.noaddr: 没有地址的节点（No address known for this node）.noflags: 连个标记都没有（No flags at all）.因为这里没有借助第三方插件的管控台，所以看懂节点的状态信息还是非常重要的，结合日志文件的说明可以更加清楚此时节点的健康状态（3）进行数据操作验证（4）关闭集群则需要逐个进行关闭，试验一下主从切换，此时7004为一个master节点，使用命令：/usr/local/redis/bin/redis-cli -c -h 10.211.55.6 -p 7004  shutdown<img src="/2018/11/24/redis总结/%E5%81%9C%E6%8E%897004%E4%B8%BB%E8%8A%82%E7%82%B9%E5%90%8E.jpg" alt="1.jpg"> 停掉7004主节点后：注意一下变化7004目前时fail状态，原本是slave的7001此时变为了master，原本7004之上的slot 66-5460移动到了7001，下面是切换的日志</p><p><img src="/2018/11/24/redis总结/%E6%97%A5%E5%BF%97%E6%98%BE%E7%A4%BA%E5%88%87%E6%8D%A2%E6%88%90%E5%8A%9F.jpg" alt="1.jpg">重启7004后，7004重新以slave节点的状态加入集群，并被挂载到7001主节点下，同时开启子线程恢复宕机期间的数据,看下图cluster nodes和日志输出<img src="/2018/11/24/redis总结/%E9%87%8D%E5%90%AF7004%E5%90%8E.jpg" alt="1.jpg"><img src="/2018/11/24/redis总结/%E6%81%A2%E5%A4%8D%E6%97%A5%E5%BF%97.jpg" alt="1.jpg">第八步：（补充）友情提示：当出现集群无法启动时，删除临时的数据文件，再次重新启动每一个redis服务，然后重新构造集群环境。</p><p>第九步：（集群操作文章）redis-trib.rb官方群操作命令: http://redis.io/topics/cluster-tutorial推荐博客: http://blog.51yip.com/nosql/1726.html/comment-page-1</p><h2>I/O 多路复用</h2><p>异步  类型于nginx的epoll 负载均衡的实现。核心的理念了，不充分了解的话感觉对redis还是只停留在应用层面，目前还不是很懂，可以参考下https://blog.csdn.net/happy_wu/article/details/80052617</p><h2>事务</h2><p>在Redis采用了线程封闭的方式，把任务封闭在一个线程，自然避免了线程安全问题，不过对于需要依赖多个redis操作的复合操作来说，依然可能需要事务,它是是一组命令的集合。Redis事务的实现需要用到 MULTI 和 EXEC 两个命令，事务开始的时候先向Redis服务器发送 MULTI 命令，然后依次发送需要在本次事务中处理的命令，最后再发送 EXEC 命令表示事务命令结束</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">    &lt;property name=&quot;enableTransactionSupport&quot; value=&quot;true&quot;&gt;</span><br><span class="line"></span><br><span class="line">    public void useTransaction(boolean use) &#123;</span><br><span class="line">        redisTemplate.setEnableTransactionSupport(use);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 开启事务   一个begin只能对应一个commit或者rollback，用完即销毁</span><br><span class="line">     */</span><br><span class="line">    public void beginTransaction() &#123;</span><br><span class="line">        redisTemplate.multi();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 同在管控台操作不通，管控台开启multi，数据进入QUEUED阶段，必须执行exec才会刷盘成功</span><br><span class="line">     * 客户端连接时，不进行exec，会默认提交所有开启multi的数据</span><br><span class="line">     */</span><br><span class="line">    public void commit() &#123;</span><br><span class="line">        redisTemplate.exec();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void rollback() &#123;</span><br><span class="line">        redisTemplate.discard();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 在finally中记得释放资源，不要执行redisTemplate.exec();</span><br><span class="line">     * 如果事务异常进入catch中，catch中的discard会消耗掉multi，此时exec会找不到对应的multi，报错，事务失败</span><br><span class="line">     */</span><br><span class="line">    public void close() &#123;</span><br><span class="line">//        redisTemplate.exec();</span><br><span class="line">        RedisConnectionUtils.unbindConnection(redisTemplate.getConnectionFactory());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>test：</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public void test() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            redisUtil.useTransaction(true);</span><br><span class="line"></span><br><span class="line">            redisUtil.beginTransaction();</span><br><span class="line">            redisUtil.set(&quot;name1&quot;, &quot;xiemao1111&quot;);</span><br><span class="line">//            redisUtil.commit();</span><br><span class="line">//            int a = 1 / 0;</span><br><span class="line">            redisUtil.set(&quot;name2&quot;, &quot;xiemao2222&quot;);</span><br><span class="line">            redisUtil.commit();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            redisUtil.rollback();</span><br><span class="line">            logger.error(&quot;error:&quot;,e);</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            redisUtil.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><h2>缓存淘汰策略：</h2><p>redis.conf中的 maxmemory-policy volatile-lru1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。推荐使用。3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。应该也没人用吧，你不删最少使用Key,去随机删。4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。不推荐。5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。依然不推荐。6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐。</p><h2>mysql数据库与redis数据的双写不一致问题：</h2><p>     这个问题其实也是蛮重要的，这样几个问题，是先更新db还是先更新redis？是更新缓存还是删掉之后重新新增？两者之间怎样选择合理的同步模式？个人给出自己的一点理解：对于读的场景，先去读redis，命中直接返回，没命中去读mysql，同时将mysql的数据放入到redis中之后再返回。对于写的场景（两者数据要异步串行化），先删掉redis的缓存，然后更新mysql，mysql更新成功后再写redis，这样比直接修改缓存容错率会高很多，因为这种方式即使最后写入缓存失败，缓存为空，下次读请求进来，也会在mysql中命中，而不会产生脏数据。并且在某些写场景特别复杂频繁并且此时读请求少的情况下，使用这种方式可以有效避免频繁的写缓存的情况，这种解决方案其实应用的是一种懒加载的思想来进行容错。如果这里先更新数据库，再删除缓存，前面成功后面失败，下次从redis读到的就还是旧数据了。https://blog.csdn.net/simba_1986/article/details/77823309#commentBox  可以参考下这个博客</p><h2>实际应用场景，自己的一点体会：</h2><p>     1.应用服务器做到无状态：session存储在redis中，做单点登录的时候，就是用hash存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。     2.mysql数据的缓存，接口数据的缓存，最常用也最为常见的功能，有各种各样的选择方式，举个例子，service服务等包含更新操作，说到幂等性，对于分布式场景，接口一方面要提供超时机制，一方面要保证接口的可重入性，也就是保证接口的幂等性，可以通过mysql的唯一索引重复插入抛出异常进行回滚控制，也可以借助redis生成唯一标示，标示接口被调用过，防止超时后的重复请求问题。     3.消息队列，不建议，毕竟不是专业的，数据不丢失（虽然有事务机制，但做到像mq那样保证生产者的事务回滚机制，消费者的去阻塞堆积的功能，广播消息的实现还是比较复杂的）业务数据有专业的mq，流式数据有kafka等，没必要用redis。     4.全局唯一标志，订单号，流水号等需唯一的数据，redis可充当发号器的角色，如在代码中生成一个订单号，保证全局唯一,这时可用redis的setnx，前缀为业务标示加上用户id，如&quot;xxxdb&quot;+&quot;uid&quot;+当前时间戳通过setnx操作，返回值进行判断，如果为1，让入成功，不为1，让入失败，while循环着。记得设置一天的过期时间。     5.全局的数据，全局的单例模式的概念只是在当前jvm下的单例，生成分布式环境下唯一资源对象或者是对统一资源进行并发的修改更新操作等，如创建一个如秒杀商品扣库存等操作。秒杀的一种实现思路，redis中的库存和mysql同步的问题：写请求过来，放入队列（kafka集群），读请求读redis，有直接返回，无，放入队列中。队列中执行读请求，先读redis，redis为空，去mysql拿最新数据（此时这种队列中的10个读中间无更新的连续读，实际会只读一次mysqk，因为第一次后redis中会存在最新数据了，可以直接返回），放入redis，redis不为空，直接返回，队列中执行写请求，删redis，mysql扣库存。     6.分布式锁，实现方式很多种，如：(1)数据库的版本号，加个version字段用乐观锁的方式。(2)zk的临时节点，进来的时候判断下有没有临时节点，没有创建一个获得执行权，执行完删除临时节点，有的话说明资源被抢占，阻塞等待。(3)redis实现分布式锁，其实有各种各样的方式，我目前理解的好像还没有绝对安全的方式，但是有几种比较典型的错误方式如破坏setnx的原子性操作，客户端时钟与服务器时钟不同步的问题，集群的子线程io异步同步数据在不稳定的情况下可能导致的重复锁的等等问题，可以参考下https://mp.weixin.qq.com/s/8FYMUpaBcgOZ9lEqt-0PKg 这篇文章，写得还可以，对于java两种客户端jedis和stringRedisTemplate，我之前一直都是这么使用的：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) &#123;</span><br><span class="line">        String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime);</span><br><span class="line">        if (LOCK_SUCCESS.equals(result)) &#123;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>释放锁应该在finally里面进行释放，避免代码异常索得不到释放<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stringRedisTemplate.opsForValue().setIfAbsent(redisKey, startTime.toString());</span><br><span class="line">stringRedisTemplate.delete(redisKey);</span><br></pre></td></tr></table></figure></p><p>     7.缓存时间的设置，缓存雪崩：即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常处理：参考方案一：一个批量操作，设置expire time时，在expire time的基础上给缓存的失效时间加上一个随机值（1到5分钟），避免集体失效。参考方案二：双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点：     I 从缓存A读数据库，有则直接返回     II A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。     III 更新线程同时更新缓存A和缓存B。还是推荐1吧，2需要维护两份缓存，麻烦<br>     8.缓存穿透（1）布隆过滤器：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力（2）如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟</p><p>     9.在cluster模式下，对mset  mget命令限制很多，要求批量设置的key 都在同一台redis实例上，否则报异常解决：不建议用mset等，用hashmap转为json存储，hashmap中的元素增加变为json的覆盖。<br>     10.绝大多数命令的时间复杂度都为O(1) ,对于keys *  ,flush all这样n时间复杂度的操作在生产环境也应该被禁止掉，执行这一的操作因数据的大小而时间不可控，会造成线程的阻塞，大量请求被堆积。     11. 主从复制的延迟问题很容易发生，可以通过shell脚本做定时监控报警，思路：cluster info中offset，master和slave复制的offset，做一个差值就可以看到对应的延迟量，如果延迟过多，那么就进行报警。     12. master宕机，集群心跳检测还没检测到（更谈不上切换的过程了），期间写入失败的问题，可以在客户端加保护措施的，写入失败，将数据丢入kafka等消息队列中，定时任务监控重新建立了master，再将期间的数据从kafka写入到新的master节点上</p><h2>配置文件参考：</h2><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br></pre></td><td class="code"><pre><span class="line"># Redis 配置文件</span><br><span class="line"></span><br><span class="line"># 当配置中需要配置内存大小时，可以使用 1k, 5GB, 4M 等类似的格式，其转换方式如下(不区分大小写)</span><br><span class="line">#</span><br><span class="line"># 1k =&gt;</span><br><span class="line">1000 bytes</span><br><span class="line"># 1kb =&gt; 1024 bytes</span><br><span class="line"># 1m =&gt; 1000000 bytes</span><br><span class="line"># 1mb =&gt;</span><br><span class="line">1024*1024 bytes</span><br><span class="line"># 1g =&gt; 1000000000 bytes</span><br><span class="line"># 1gb =&gt; 1024*1024*1024</span><br><span class="line">bytes</span><br><span class="line">#</span><br><span class="line"># 内存配置大小写是一样的.比如 1gb 1Gb 1GB 1gB</span><br><span class="line"></span><br><span class="line"># daemonize no 默认情况下，redis不是在后台运行的，如果需要在后台运行，把该项的值更改为yes</span><br><span class="line">daemonize</span><br><span class="line">yes</span><br><span class="line"></span><br><span class="line"># 当redis在后台运行的时候，Redis默认会把pid文件放在/var/run/redis.pid，你可以配置到其他地址。</span><br><span class="line">#</span><br><span class="line">当运行多个redis服务时，需要指定不同的pid文件和端口</span><br><span class="line">pidfile /var/run/redis.pid</span><br><span class="line"></span><br><span class="line"># 指定redis运行的端口，默认是6379</span><br><span class="line">port 6379</span><br><span class="line"></span><br><span class="line"># 指定redis只接收来自于该IP地址的请求，如果不进行设置，那么将处理所有请求，</span><br><span class="line"># 在生产环境中最好设置该项</span><br><span class="line"># bind</span><br><span class="line">127.0.0.1</span><br><span class="line"></span><br><span class="line"># Specify the path for the unix socket that will be used to listen for</span><br><span class="line">#</span><br><span class="line">incoming connections. There is no default, so Redis will not listen</span><br><span class="line"># on a</span><br><span class="line">unix socket when not specified.</span><br><span class="line">#</span><br><span class="line"># unixsocket /tmp/redis.sock</span><br><span class="line">#</span><br><span class="line">unixsocketperm 755</span><br><span class="line"></span><br><span class="line"># 设置客户端连接时的超时时间，单位为秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接</span><br><span class="line"># 0是关闭此设置</span><br><span class="line">timeout</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"># 指定日志记录级别</span><br><span class="line"># Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose</span><br><span class="line">#</span><br><span class="line">debug  记录很多信息，用于开发和测试</span><br><span class="line"># varbose 有用的信息，不像debug会记录那么多</span><br><span class="line">#</span><br><span class="line">notice 普通的verbose，常用于生产环境</span><br><span class="line"># warning 只有非常重要或者严重的信息会记录到日志</span><br><span class="line">loglevel</span><br><span class="line">debug</span><br><span class="line"></span><br><span class="line"># 配置log文件地址</span><br><span class="line"># 默认值为stdout，标准输出，若后台模式会输出到/dev/null</span><br><span class="line">#logfile</span><br><span class="line">stdout</span><br><span class="line">logfile /var/log/redis/redis.log</span><br><span class="line"></span><br><span class="line"># To enable logging to the system logger, just set &apos;syslog-enabled&apos; to</span><br><span class="line">yes,</span><br><span class="line"># and optionally update the other syslog parameters to suit your</span><br><span class="line">needs.</span><br><span class="line"># syslog-enabled no</span><br><span class="line"></span><br><span class="line"># Specify the syslog identity.</span><br><span class="line"># syslog-ident redis</span><br><span class="line"></span><br><span class="line"># Specify the syslog facility.  Must be USER or between LOCAL0-LOCAL7.</span><br><span class="line">#</span><br><span class="line">syslog-facility local0</span><br><span class="line"></span><br><span class="line"># 可用数据库数</span><br><span class="line"># 默认值为16，默认数据库为0，数据库范围在0-（database-1）之间</span><br><span class="line">databases 16</span><br><span class="line"></span><br><span class="line">################################ 快照  </span><br><span class="line">#################################</span><br><span class="line">#</span><br><span class="line"># 保存数据到磁盘，格式如下:</span><br><span class="line">#</span><br><span class="line">#   save</span><br><span class="line">&lt;seconds&gt; &lt;changes&gt;</span><br><span class="line">#</span><br><span class="line">#   </span><br><span class="line">指出在多长时间内，有多少次更新操作，就将数据同步到数据文件rdb。</span><br><span class="line">#   相当于条件触发抓取快照，这个可以多个条件配合</span><br><span class="line">#   </span><br><span class="line">#   </span><br><span class="line">比如默认配置文件中的设置，就设置了三个条件</span><br><span class="line">#</span><br><span class="line">#   save 900 1  900秒内至少有1个key被改变</span><br><span class="line">#   save 300</span><br><span class="line">10  300秒内至少有300个key被改变</span><br><span class="line">#   save 60 10000  60秒内至少有10000个key被改变</span><br><span class="line"></span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br><span class="line"></span><br><span class="line"># 存储至本地数据库时（持久化到rdb文件）是否压缩数据，默认为yes</span><br><span class="line">rdbcompression yes</span><br><span class="line"></span><br><span class="line"># 本地持久化数据库文件名，默认值为dump.rdb</span><br><span class="line">dbfilename dump.rdb</span><br><span class="line"></span><br><span class="line"># 工作目录</span><br><span class="line">#</span><br><span class="line"># 数据库镜像备份的文件放置的路径。</span><br><span class="line">#</span><br><span class="line">这里的路径跟文件名要分开配置是因为redis在进行备份时，先会将当前数据库的状态写入到一个临时文件中，等备份完成时，</span><br><span class="line">#</span><br><span class="line">再把该该临时文件替换为上面所指定的文件，而这里的临时文件和上面所配置的备份文件都会放在这个指定的路径当中。</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">AOF文件也会存放在这个目录下面</span><br><span class="line">#</span><br><span class="line"># 注意这里必须制定一个目录而不是文件</span><br><span class="line">dir ./</span><br><span class="line"></span><br><span class="line">################################# 复制</span><br><span class="line">#################################</span><br><span class="line"></span><br><span class="line"># 主从复制. 设置该数据库为其他数据库的从数据库.</span><br><span class="line">#</span><br><span class="line">设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步</span><br><span class="line">#</span><br><span class="line"># slaveof</span><br><span class="line">&lt;masterip&gt; &lt;masterport&gt;</span><br><span class="line"></span><br><span class="line"># 当master服务设置了密码保护时(用requirepass制定的密码)</span><br><span class="line"># slav服务连接master的密码</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">masterauth &lt;master-password&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：</span><br><span class="line">#</span><br><span class="line"># 1)</span><br><span class="line">如果slave-serve-stale-data设置为yes(默认设置)，从库会继续相应客户端的请求</span><br><span class="line">#</span><br><span class="line"># 2)</span><br><span class="line">如果slave-serve-stale-data是指为no，出去INFO和SLAVOF命令之外的任何请求都会返回一个</span><br><span class="line">#    错误&quot;SYNC with</span><br><span class="line">master in progress&quot;</span><br><span class="line">#</span><br><span class="line">slave-serve-stale-data yes</span><br><span class="line"></span><br><span class="line"># 从库会按照一个时间间隔向主库发送PINGs.可以通过repl-ping-slave-period设置这个时间间隔，默认是10秒</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">repl-ping-slave-period 10</span><br><span class="line"></span><br><span class="line"># repl-timeout 设置主库批量数据传输时间或者ping回复时间间隔，默认值是60秒</span><br><span class="line">#</span><br><span class="line">一定要确保repl-timeout大于repl-ping-slave-period</span><br><span class="line"># repl-timeout 60</span><br><span class="line"></span><br><span class="line">################################## 安全</span><br><span class="line">###################################</span><br><span class="line"></span><br><span class="line"># 设置客户端连接后进行任何其他指定前需要使用的密码。</span><br><span class="line">#</span><br><span class="line">警告：因为redis速度相当快，所以在一台比较好的服务器下，一个外部的用户可以在一秒钟进行150K次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">requirepass foobared</span><br><span class="line"></span><br><span class="line"># 命令重命名.</span><br><span class="line">#</span><br><span class="line"># 在一个共享环境下可以重命名相对危险的命令。比如把CONFIG重名为一个不容易猜测的字符。</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">举例:</span><br><span class="line">#</span><br><span class="line"># rename-command CONFIG</span><br><span class="line">b840fc02d524045429941cc15f59e41cb7be6c52</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">如果想删除一个命令，直接把它重命名为一个空字符&quot;&quot;即可，如下：</span><br><span class="line">#</span><br><span class="line"># rename-command CONFIG &quot;&quot;</span><br><span class="line"></span><br><span class="line">################################### 约束</span><br><span class="line">####################################</span><br><span class="line"></span><br><span class="line"># 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，</span><br><span class="line"># 如果设置</span><br><span class="line">maxclients 0，表示不作限制。</span><br><span class="line"># 当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients</span><br><span class="line">reached错误信息</span><br><span class="line">#</span><br><span class="line"># maxclients 128</span><br><span class="line"></span><br><span class="line"># 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key</span><br><span class="line">#</span><br><span class="line">Redis同时也会移除空的list对象</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">注意：Redis新的vm机制，会把Key存放内存，Value会存放在swap区</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">maxmemory的设置比较适合于把redis当作于类似memcached的缓存来使用，而不适合当做一个真实的DB。</span><br><span class="line">#</span><br><span class="line">当把Redis当做一个真实的数据库使用的时候，内存使用将是一个很大的开销</span><br><span class="line"># maxmemory &lt;bytes&gt;</span><br><span class="line"></span><br><span class="line"># 当内存达到最大值的时候Redis会选择删除哪些数据？有五种方式可供选择</span><br><span class="line">#</span><br><span class="line"># volatile-lru -&gt;</span><br><span class="line">利用LRU算法移除设置过过期时间的key (LRU:最近使用 Least Recently Used )</span><br><span class="line"># allkeys-lru -&gt;</span><br><span class="line">利用LRU算法移除任何key</span><br><span class="line"># volatile-random -&gt; 移除设置过过期时间的随机key</span><br><span class="line">#</span><br><span class="line">allkeys-&gt;random -&gt; remove a random key, any key</span><br><span class="line"># volatile-ttl -&gt;</span><br><span class="line">移除即将过期的key(minor TTL)</span><br><span class="line"># noeviction -&gt; 不移除任何可以，只是返回一个写错误</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">注意：对于上面的策略，如果没有合适的key可以移除，当写的时候Redis会返回一个错误</span><br><span class="line">#</span><br><span class="line">#       写命令包括: set setnx</span><br><span class="line">setex append</span><br><span class="line">#       incr decr rpush lpush rpushx lpushx linsert lset</span><br><span class="line">rpoplpush sadd</span><br><span class="line">#       sinter sinterstore sunion sunionstore sdiff sdiffstore</span><br><span class="line">zadd zincrby</span><br><span class="line">#       zunionstore zinterstore hset hsetnx hmset hincrby incrby</span><br><span class="line">decrby</span><br><span class="line">#       getset mset msetnx exec sort</span><br><span class="line">#</span><br><span class="line"># 默认是:</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">maxmemory-policy volatile-lru</span><br><span class="line"></span><br><span class="line"># LRU 和 minimal TTL 算法都不是精准的算法，但是相对精确的算法(为了节省内存)，随意你可以选择样本大小进行检测。</span><br><span class="line">#</span><br><span class="line">Redis默认的灰选择3个样本进行检测，你可以通过maxmemory-samples进行设置</span><br><span class="line">#</span><br><span class="line"># maxmemory-samples</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">############################## AOF ###############################</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">默认情况下，redis会在后台异步的把数据库镜像备份到磁盘，但是该备份是非常耗时的，而且备份也不能很频繁，如果发生诸如拉闸限电、拔插头等状况，那么将造成比较大范围的数据丢失。</span><br><span class="line">#</span><br><span class="line">所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式。</span><br><span class="line"># 开启append</span><br><span class="line">only模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态。</span><br><span class="line">#</span><br><span class="line">但是这样会造成appendonly.aof文件过大，所以redis还支持了BGREWRITEAOF指令，对appendonly.aof 进行重新整理。</span><br><span class="line">#</span><br><span class="line">你可以同时开启asynchronous dumps 和 AOF</span><br><span class="line"></span><br><span class="line">appendonly no</span><br><span class="line"></span><br><span class="line"># AOF文件名称 (默认: &quot;appendonly.aof&quot;)</span><br><span class="line"># appendfilename appendonly.aof</span><br><span class="line"></span><br><span class="line"># Redis支持三种同步AOF文件的策略:</span><br><span class="line">#</span><br><span class="line"># no: 不进行同步，系统去操作 . Faster.</span><br><span class="line"># always:</span><br><span class="line">always表示每次有写操作都进行同步. Slow, Safest.</span><br><span class="line"># everysec: 表示对写操作进行累积，每秒同步一次.</span><br><span class="line">Compromise.</span><br><span class="line">#</span><br><span class="line"># 默认是&quot;everysec&quot;，按照速度和安全折中这是最好的。</span><br><span class="line">#</span><br><span class="line">如果想让Redis能更高效的运行，你也可以设置为&quot;no&quot;，让操作系统决定什么时候去执行</span><br><span class="line">#</span><br><span class="line">或者相反想让数据更安全你也可以设置为&quot;always&quot;</span><br><span class="line">#</span><br><span class="line"># 如果不确定就用 &quot;everysec&quot;.</span><br><span class="line"></span><br><span class="line"># appendfsync always</span><br><span class="line">appendfsync everysec</span><br><span class="line"># appendfsync no</span><br><span class="line"></span><br><span class="line"># AOF策略设置为always或者everysec时，后台处理进程(后台保存或者AOF日志重写)会执行大量的I/O操作</span><br><span class="line">#</span><br><span class="line">在某些Linux配置中会阻止过长的fsync()请求。注意现在没有任何修复，即使fsync在另外一个线程进行处理</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">为了减缓这个问题，可以设置下面这个参数no-appendfsync-on-rewrite</span><br><span class="line">#</span><br><span class="line"># This means that while</span><br><span class="line">another child is saving the durability of Redis is</span><br><span class="line"># the same as &quot;appendfsync</span><br><span class="line">none&quot;, that in pratical terms means that it is</span><br><span class="line"># possible to lost up to 30</span><br><span class="line">seconds of log in the worst scenario (with the</span><br><span class="line"># default Linux</span><br><span class="line">settings).</span><br><span class="line">#</span><br><span class="line"># If you have latency problems turn this to &quot;yes&quot;. Otherwise</span><br><span class="line">leave it as</span><br><span class="line"># &quot;no&quot; that is the safest pick from the point of view of</span><br><span class="line">durability.</span><br><span class="line">no-appendfsync-on-rewrite no</span><br><span class="line"></span><br><span class="line"># Automatic rewrite of the append only file.</span><br><span class="line"># AOF 自动重写</span><br><span class="line">#</span><br><span class="line">当AOF文件增长到一定大小的时候Redis能够调用 BGREWRITEAOF 对日志文件进行重写</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">它是这样工作的：Redis会记住上次进行些日志后文件的大小(如果从开机以来还没进行过重写，那日子大小在开机的时候确定)</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">基础大小会同现在的大小进行比较。如果现在的大小比基础大小大制定的百分比，重写功能将启动</span><br><span class="line">#</span><br><span class="line">同时需要指定一个最小大小用于AOF重写，这个用于阻止即使文件很小但是增长幅度很大也去重写AOF文件的情况</span><br><span class="line"># 设置 percentage</span><br><span class="line">为0就关闭这个特性</span><br><span class="line"></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line"></span><br><span class="line">################################## SLOW LOG</span><br><span class="line">###################################</span><br><span class="line"></span><br><span class="line"># Redis Slow Log 记录超过特定执行时间的命令。执行时间不包括I/O计算比如连接客户端，返回结果等，只是命令执行时间</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">可以通过两个参数设置slow log：一个是告诉Redis执行超过多少时间被记录的参数slowlog-log-slower-than(微妙)，</span><br><span class="line">#</span><br><span class="line">另一个是slow log 的长度。当一个新命令被记录的时候最早的命令将被从队列中移除</span><br><span class="line"></span><br><span class="line"># 下面的时间以微妙微单位，因此1000000代表一分钟。</span><br><span class="line">#</span><br><span class="line">注意制定一个负数将关闭慢日志，而设置为0将强制每个命令都会记录</span><br><span class="line">slowlog-log-slower-than 10000</span><br><span class="line"></span><br><span class="line"># 对日志长度没有限制，只是要注意它会消耗内存</span><br><span class="line"># 可以通过 SLOWLOG RESET</span><br><span class="line">回收被慢日志消耗的内存</span><br><span class="line">slowlog-max-len 1024</span><br><span class="line"></span><br><span class="line">################################ VM ###############################</span><br><span class="line"></span><br><span class="line">### WARNING! Virtual Memory is deprecated in Redis 2.4</span><br><span class="line">### The use of</span><br><span class="line">Virtual Memory is strongly discouraged.</span><br><span class="line"></span><br><span class="line"># Virtual Memory allows Redis to work with datasets bigger than the</span><br><span class="line">actual</span><br><span class="line"># amount of RAM needed to hold the whole dataset in memory.</span><br><span class="line"># In</span><br><span class="line">order to do so very used keys are taken in memory while the other keys</span><br><span class="line"># are</span><br><span class="line">swapped into a swap file, similarly to what operating systems do</span><br><span class="line"># with</span><br><span class="line">memory pages.</span><br><span class="line">#</span><br><span class="line"># To enable VM just set &apos;vm-enabled&apos; to yes, and set the</span><br><span class="line">following three</span><br><span class="line"># VM parameters accordingly to your needs.</span><br><span class="line"></span><br><span class="line">vm-enabled no</span><br><span class="line"># vm-enabled yes</span><br><span class="line"></span><br><span class="line"># This is the path of the Redis swap file. As you can guess, swap</span><br><span class="line">files</span><br><span class="line"># can&apos;t be shared by different Redis instances, so make sure to use a</span><br><span class="line">swap</span><br><span class="line"># file for every redis process you are running. Redis will complain if</span><br><span class="line">the</span><br><span class="line"># swap file is already in use.</span><br><span class="line">#</span><br><span class="line"># The best kind of storage for the</span><br><span class="line">Redis swap file (that&apos;s accessed at random)</span><br><span class="line"># is a Solid State Disk</span><br><span class="line">(SSD).</span><br><span class="line">#</span><br><span class="line"># *** WARNING *** if you are using a shared hosting the default</span><br><span class="line">of putting</span><br><span class="line"># the swap file under /tmp is not secure. Create a dir with access</span><br><span class="line">granted</span><br><span class="line"># only to Redis user and configure Redis to create the swap file</span><br><span class="line">there.</span><br><span class="line">vm-swap-file /tmp/redis.swap</span><br><span class="line"></span><br><span class="line"># vm-max-memory configures the VM to use at max the specified amount</span><br><span class="line">of</span><br><span class="line"># RAM. Everything that deos not fit will be swapped on disk *if* possible,</span><br><span class="line">that</span><br><span class="line"># is, if there is still enough contiguous space in the swap</span><br><span class="line">file.</span><br><span class="line">#</span><br><span class="line"># With vm-max-memory 0 the system will swap everything it can. Not</span><br><span class="line">a good</span><br><span class="line"># default, just specify the max amount of RAM you can in bytes, but</span><br><span class="line">it&apos;s</span><br><span class="line"># better to leave some margin. For instance specify an amount of</span><br><span class="line">RAM</span><br><span class="line"># that&apos;s more or less between 60 and 80% of your free</span><br><span class="line">RAM.</span><br><span class="line">vm-max-memory 0</span><br><span class="line"></span><br><span class="line"># Redis swap files is split into pages. An object can be saved using</span><br><span class="line">multiple</span><br><span class="line"># contiguous pages, but pages can&apos;t be shared between different</span><br><span class="line">objects.</span><br><span class="line"># So if your page is too big, small objects swapped out on disk will</span><br><span class="line">waste</span><br><span class="line"># a lot of space. If you page is too small, there is less space in the</span><br><span class="line">swap</span><br><span class="line"># file (assuming you configured the same number of total swap file</span><br><span class="line">pages).</span><br><span class="line">#</span><br><span class="line"># If you use a lot of small objects, use a page size of 64 or 32</span><br><span class="line">bytes.</span><br><span class="line"># If you use a lot of big objects, use a bigger page size.</span><br><span class="line"># If</span><br><span class="line">unsure, use the default :)</span><br><span class="line">vm-page-size 32</span><br><span class="line"></span><br><span class="line"># Number of total memory pages in the swap file.</span><br><span class="line"># Given that the page</span><br><span class="line">table (a bitmap of free/used pages) is taken in memory,</span><br><span class="line"># every 8 pages on</span><br><span class="line">disk will consume 1 byte of RAM.</span><br><span class="line">#</span><br><span class="line"># The total swap size is vm-page-size *</span><br><span class="line">vm-pages</span><br><span class="line">#</span><br><span class="line"># With the default of 32-bytes memory pages and 134217728 pages</span><br><span class="line">Redis will</span><br><span class="line"># use a 4 GB swap file, that will use 16 MB of RAM for the page</span><br><span class="line">table.</span><br><span class="line">#</span><br><span class="line"># It&apos;s better to use the smallest acceptable value for your</span><br><span class="line">application,</span><br><span class="line"># but the default is large in order to work in most</span><br><span class="line">conditions.</span><br><span class="line">vm-pages 134217728</span><br><span class="line"></span><br><span class="line"># Max number of VM I/O threads running at the same time.</span><br><span class="line"># This threads</span><br><span class="line">are used to read/write data from/to swap file, since they</span><br><span class="line"># also encode and</span><br><span class="line">decode objects from disk to memory or the reverse, a bigger</span><br><span class="line"># number of</span><br><span class="line">threads can help with big objects even if they can&apos;t help with</span><br><span class="line"># I/O itself</span><br><span class="line">as the physical device may not be able to couple with many</span><br><span class="line"># reads/writes</span><br><span class="line">operations at the same time.</span><br><span class="line">#</span><br><span class="line"># The special value of 0 turn off threaded</span><br><span class="line">I/O and enables the blocking</span><br><span class="line"># Virtual Memory</span><br><span class="line">implementation.</span><br><span class="line">vm-max-threads 4</span><br><span class="line"></span><br><span class="line">############################### ADVANCED CONFIG</span><br><span class="line">###############################</span><br><span class="line"></span><br><span class="line"># 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，</span><br><span class="line">#</span><br><span class="line">hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值</span><br><span class="line"># Redis</span><br><span class="line">Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，</span><br><span class="line">#</span><br><span class="line">这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value</span><br><span class="line">redisObject的encoding为zipmap,</span><br><span class="line">#</span><br><span class="line">当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。</span><br><span class="line">hash-max-zipmap-entries</span><br><span class="line">512</span><br><span class="line">hash-max-zipmap-value 64</span><br><span class="line"></span><br><span class="line"># list数据类型多少节点以下会采用去指针的紧凑存储格式。</span><br><span class="line">#</span><br><span class="line">list数据类型节点值大小小于多少字节会采用紧凑存储格式。</span><br><span class="line">list-max-ziplist-entries</span><br><span class="line">512</span><br><span class="line">list-max-ziplist-value 64</span><br><span class="line"></span><br><span class="line"># set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。</span><br><span class="line">set-max-intset-entries</span><br><span class="line">512</span><br><span class="line"></span><br><span class="line"># zsort数据类型多少节点以下会采用去指针的紧凑存储格式。</span><br><span class="line">#</span><br><span class="line">zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。</span><br><span class="line">zset-max-ziplist-entries</span><br><span class="line">128</span><br><span class="line">zset-max-ziplist-value 64</span><br><span class="line"></span><br><span class="line"># Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存</span><br><span class="line">activerehashing yes</span><br><span class="line"></span><br><span class="line">################################## INCLUDES</span><br><span class="line">###################################</span><br><span class="line"></span><br><span class="line"># 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件</span><br><span class="line">#</span><br><span class="line">include /path/to/local.conf</span><br><span class="line"># include /path/to/other.conf</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;关于redis的一点总结&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;引言&lt;/h2&gt;
&lt;p&gt;     常规的应用系统的系统瓶颈大部分都出现在数据库的IO操作，并且90%的请求基本都是select操作，对于数据库的优化手段可以有非常多的手段，举几种常见的:
&amp;lt;!--more--&amp;gt
      
    
    </summary>
    
    
      <category term="redis" scheme="http://yoursite.com/tags/redis/"/>
    
      <category term="缓存" scheme="http://yoursite.com/tags/%E7%BC%93%E5%AD%98/"/>
    
  </entry>
  
  <entry>
    <title>关于并发的一点思考</title>
    <link href="http://yoursite.com/2018/11/11/%E5%85%B3%E4%BA%8E%E5%B9%B6%E5%8F%91%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83/"/>
    <id>http://yoursite.com/2018/11/11/关于并发的一点思考/</id>
    <published>2018-11-11T10:21:44.000Z</published>
    <updated>2018-11-24T04:41:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>并发编程核心编程模块包括线程安全，线程封闭，线程调度，同步容器，并发容器，AQS，J.U.C等等常用手段有扩容，缓存，队列，应用拆分，限流，服务降级与熔断，数据库切库，分库分表，高可用部署等等并发在了解了基础的方法后本身从理论知识来讲并不难，关键是结合到实际的应用场景时，解决问题的思路和怎样选择最有效的手段，这个需要一定的经验并需要不断的总结和学习。  TODO</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;并发编程核心编程模块包括线程安全，线程封闭，线程调度，同步容器，并发容器，AQS，J.U.C等等
常用手段有扩容，缓存，队列，应用拆分，限流，服务降级与熔断，数据库切库，分库分表，高可用部署等等
并发在了解了基础的方法后本身从理论知识来讲并不难，关键是结合到实际的应用场景时
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Fastdfs分布式文件存储</title>
    <link href="http://yoursite.com/2018/11/10/Fastdfs%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
    <id>http://yoursite.com/2018/11/10/Fastdfs分布式文件存储/</id>
    <published>2018-11-10T09:09:30.000Z</published>
    <updated>2018-11-10T17:12:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Fastdfs分布式文件存储</h1><hr><h2>简介</h2><p>     Fastdfs是一款比较轻量级的开源分布式文件系统，作者是余庆大大在0几年业余时间用c撸出来的(工作不饱和，加班不够啊:)  ，现任淘宝网开放平台技术部资深架构师,Fastdfs在小文件存储方面性能优异，在淘宝和京东都有实例的应用案例，去哪儿目前使用的是seaweedfs，基于Facebook的图片存储论文用go语言开发的一套存储系统，使用起来也是非常方便，相比Fastdfs更加现代化，这个日后研究，直接进入正题。&lt;!--more--&gt;</p><h2>相关术语</h2><p>• Tracker Server：跟踪服务器，主要负责任务调度,请求分发负载的工作。记录storage server的状态，是连接Client和Storage server的枢纽。• Storage Server：存储服务器，文件和meta data都保存到存储服务器上。• group：组，也可称为卷。同组内服务器上的文件是完全相同的。• 文件标识：包括两部分：组名和文件名（包含路径）。• meta data：文件相关属性，键值对（Key Value Pair）方式，如：width=1024,heigth=768</p><h2>系统架构</h2><p><img src="/2018/11/10/Fastdfs分布式文件存储/1.jpg" alt="1.jpg"></p><p>从图中可以看出两大核心部分，trakcer跟踪器，storage存储节点，跟踪器主要负责路由分发，任务调度，与clent端直接交互，storage则负责存储实际的数据，两部分都支持主流的分布式主从配置，即master/slave模式，在请求量大时可加大trakcer集群规模，在存储大文件或读写频繁时可加大storage集群规模，非常容易的就能做到局部和全局的横向扩展，如果对这两部分理解还很模糊我们看下文件上传下载的流程图</p><p><img src="/2018/11/10/Fastdfs分布式文件存储/2.jpg" alt="2.jpg">• 1. client询问tracker上传到的storage，不需要附加参数；• 2.tracker返回一台可用的storage；• 3.client直接和storage通讯完成文件上传。</p><p><img src="/2018/11/10/Fastdfs分布式文件存储/3.jpg" alt="3.jpg">• 1. client询问tracker下载文件的storage，参数为文件标识（组名和文件名）；• 2.tracker返回一台可用的storage；• 3.client直接和storage通讯完成文件下载。</p><h2>同步机制</h2><p>•同一组内的storage server之间是对等的，文件上传、删除等操作可以在任意一台storage server上进行；•文件同步只在同组内的storage server之间进行，采用push方式，即源服务器同步给目标服务器；•源头数据才需要同步，备份数据不需要再次同步，否则就构成环路了；•上述第二条规则有个例外，就是新增加一台storage server时，由已有的一台storage server将已有的所有数据（包括源头数据和备份数据）同步给该新增服务器，当某台master storage宕机时，依据选举算法此master节点下的某台slave storage会自动切换为master，并将原master剔除集群，原master重启后，会自动以slave的角色重新加入集群，并在重启后将宕机期间的数据自动同步到此节点上。</p><h2>文件目录结构</h2><p><img src="/2018/11/10/Fastdfs分布式文件存储/4.jpg" alt="4.jpg"><img src="/2018/11/10/Fastdfs分布式文件存储/5.jpg" alt="5.jpg"></p><h2>运行与安装（单节点）</h2><p>一、准备工作(俩台机器同时进行)<br>1.下载软件: http://sourceforge.net/projects/fastdfs/files/<br>2安装gcc。命令:yum install make cmake gcc gcc-c++</p><p>3 安装libfastcommon(俩台机器同时进行)<br>  3.1 上传libfastcommon-master.zip到/usr/local/software下<br>  3.2 进行解压libfastcommon-master.zip:<br>命令:unzip libfastcommon-master.zip -d /usr/local/fast/<br>  3.3 进入目录:cd /usr/local/fast/libfastcommon-master/<br><img src="/2018/11/10/Fastdfs分布式文件存储/6.jpg" alt="6.jpg"></p><p>4 进行编译和安装:<br>命令:  ./make.sh<br>命令:  ./make.sh install<br><img src="/2018/11/10/Fastdfs分布式文件存储/7.jpg" alt="7.jpg">注意安装的路径:也就是说,我们的libfastcommon默认安装到了/usr/lib64/这个位置。</p><p>5 进行软件创建。FastDFS主程序设置的目录为/usr/local/lib/,所以我们需要创建/usr/lib64/下的一些核心执行程序的软连接文件。<br>命令:mk dir /usr/local/lib/<br>命令:ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so<br><img src="/2018/11/10/Fastdfs分布式文件存储/8.jpg" alt="8.jpg">命令:ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so<br>命令:ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so<br>命令:ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so<br>6、安装FastDFS<br>  6.1 进入到cd /usr/local/software下,解压FastDFS_v5.05.tar.gz文件<br>  命令:cd /usr/local/software<br>  命令:tar -zxvf FastDFS_v5.05.tar.gz -C /usr/local/fast/<br>  6.2安装编译<br>  命令:cd /usr/local/fast/FastDFS/<br>  编译命令:./make.sh<br>  安装命令:./make.sh install<br><img src="/2018/11/10/Fastdfs分布式文件存储/9.jpg" alt="9.jpg">7 采用默认安装方式脚本文件说明:<br>1、服务脚本在:<br>/etc/init.d/fdfs_storaged<br>/etc/init.d/fdfs_trackerd<br><img src="/2018/11/10/Fastdfs分布式文件存储/10.jpg" alt="10.jpg">2、配置文件在:<br>/etc/fdfs/client.conf.sample<br>/etc/fdfs/storage.conf.sample<br>/etc/fdfs/tracker.conf.sample<br><img src="/2018/11/10/Fastdfs分布式文件存储/11.jpg" alt="11.jpg">3、命令行工具在/usr/bin/目录下Fdfs_*的一些列执行脚本<img src="/2018/11/10/Fastdfs分布式文件存储/12.jpg" alt="12.jpg">4 因为FastDFS服务脚本设置的bin目录为/usr/local/bin/下,但是实际我们安装在了/usr/bin/下面。所以我们需要修改FastDFS配置文件中的路径,也就是需要修改俩个配置文件:<br>命令:vim /etc/init.d/fdfs_storaged<br>进行全局替换命令:%s+/usr/local/bin+/usr/bin<br>命令:vim /etc/init.d/fdfs_trackerd<br>进行全局替换命令:%s+/usr/local/bin+/usr/bin<br>4、配置跟踪器(192.168.1.172节点)<br>1进入cd/etc/fdfs/目录配置跟踪器文件(注意是192.168.1.172节点),把tracker.conf.sample文件进行cope一份:去修改tracker.conf文件</p><p><img src="/2018/11/10/Fastdfs分布式文件存储/13.jpg" alt="13.jpg"></p><p>2 修改tracker.conf文件<br>命令:vim /etc/fdfs/tracker.conf<br>如下图所示:我们暂时修改配置文件里的base_path即可。<br><img src="/2018/11/10/Fastdfs分布式文件存储/14.jpg" alt="14.jpg"></p><p>修改为自己的路径地址:base_path=/fastdfs/tracker<br>注意:对于tracker.conf配置文件参数解释可以找官方文档,地址为:<br>http://bbs.chinaunix.net/thread-1941456-1-1.html<br>3 最后我们一定要创建之前定义好的目录(也就是/fastdfs/tracker):<br>命令:mkdir -p /fastdfs/tracker<br>4 关闭防火墙:(我们在学习时可以不用考虑防火墙的问题)<br>vim /etc/sysconfig/iptables<br>添加:-A INPUT -m state --state NEW -m tcp -p tcp --dport 22122 -j ACCEPT<br>重启:service iptables restart<br>5 启动跟踪器<br>如图所示:<br><img src="/2018/11/10/Fastdfs分布式文件存储/15.jpg" alt="15.jpg"></p><p>目录命令:cd /fastdfs/tracker/ &amp;&amp; ll<br>启动tracker命令:/etc/init.d/fdfs_trackerd start<br>查看进程命令:ps -el | grep fdfs<br>停止tracker命令:/etc/init.d/fdfs_trackerd stop</p><p>6可以设置开机启动跟踪器:(一般生产环境需要开机启动一些服务,如keepalived、linux、tomcat等等)<br>命令:vim /etc/rc.d/rc.local<br>加入配置:/etc/init.d/fdfs_trackerd start</p><p>5、配置FastDFS存储(192.168.1.173)<br>1 进入文件目录:cd /etc/fdfs/,进行copy storage文件一份<br>命令:cd /etc/fdfs/<br>命令:cp storage.conf.sample storage.conf<br>2 修改storage.conf文件<br>命令:vim /etc/fdfs/storage.conf<br>修改内容:<br>base_path=/fastdfs/storage<br>store_path0=/fastdfs/storage<br>tracker_server=192.168.1.172:22122<br>http.server_port=8888<br>3 创建存储目录:mkdir -p /fastdfs/storage<br>4 打开防火墙:<br>命令:vim /etc/sysconfig/iptables<br>添加:-A INPUT -m state --state NEW -m tcp -p tcp --dport 23000 -j ACCEPT<br>重启:service iptables restart<br>5 启动存储(storage)<br>命令:/etc/init.d/fdfs_storaged start (关闭:/etc/init.d/fdfs_storaged stop)<br>(初次启动成功后会在/fastdbf/storage/ 目录下创建 data、logs俩个目录)<img src="/2018/11/10/Fastdfs分布式文件存储/16.jpg" alt="16.jpg">6 查看FastDFS storage 是否启动成功<br>命令:ps -ef | grep fdfs<img src="/2018/11/10/Fastdfs分布式文件存储/17.jpg" alt="17.jpg">并且我们进入到/fastdfs/storage/data/文件夹下会看到一些目录文件(256*256),如下:<br>命令:cd /fastdfs/storage/data/ &amp;&amp; ls<br><img src="/2018/11/10/Fastdfs分布式文件存储/18.jpg" alt="18.jpg">7 同理,也可以设置开机启动存储器:(一般生产环境需要开机启动一些服务,如keepalived、linux、tomcat等等)<br>命令:vim /etc/rc.d/rc.local<br>加入配置:/etc/init.d/fdfs_storaged start<br>到此为止我们的FastDFS环境已经搭建完成。。。</p><h2>验证环境</h2><p>1 我们先使用命令上传一个文件。注意:是在tracker(跟踪器)中上传。首先我们在跟踪器(192.168.1.172)里copy一份client.conf文件。<br>命令:cd /etc/fdfs/<br>命令:cp client.conf.sample client.conf<br><img src="/2018/11/10/Fastdfs分布式文件存储/19.jpg" alt="19.jpg"></p><p>2 编辑client.conf文件<br>命令:vim /etc/fdfs/client.conf<br>修改内容:<br>base_path=/fastdfs/tracker<br>tracker_server=192.168.1.172:22122<br>3 我们找到命令的脚本位置,并且使用命令,进行文件的上传:<br>命令:cd /usr/bin/<br>命令:ls | grep fdfs<br><img src="/2018/11/10/Fastdfs分布式文件存储/20.jpg" alt="20.jpg"></p><p>4 使用命令fdfs_upload_file进行上传操作:<br>首先,我们先看一下存储器(192.168.1.173),进入到data下,在进入00文件夹下,发现00文件夹下还有一堆文件夹,然后继续进入00文件夹下,最终我们所进入的文件夹为:<br>/fastdfs/storage/data/00/00 里面什么文件都没有。</p><p><img src="/2018/11/10/Fastdfs分布式文件存储/21.jpg" alt="21.jpg"></p><p>然后,我们进行上传操作,比如把之前的/usr/local/software/文件夹下的某一个文件上传到FastDFS系统中去,在跟踪器(192.168.1.172)中上传文件,命令如下:<br>命令:/usr/bin/fdfs_upload_file /etc/fdfs/client.conf<br>/usr/local/software/FastDFS_v5.05.tar.gz<br><img src="/2018/11/10/Fastdfs分布式文件存储/22.jpg" alt="22.jpg"></p><p>最后我们发现,命令执行完毕后,返回一个group1/M00/00/00/...的ID,其实就是返回当前所上传的文件在存储器(192.168.1.173)中的哪一个组、哪一个目录位置,所以我们查看存储器中的/fastdfs/storage/data/00/00文件夹位置,发现已经存在了刚才上传的文件,到此为止,我们的测试上传文件已经OK了。如下<br><img src="/2018/11/10/Fastdfs分布式文件存储/23.jpg" alt="23.jpg"></p><p>7、FastDFS与Nginx整合<br>1 首先两台机器里必须先安装nginx<br>2然后我们在存储节点上(192.168.1.173)安装fastdfs-nginx-module_v1.16.tar.gz包进行整合。<br><img src="/2018/11/10/Fastdfs分布式文件存储/24.jpg" alt="24.jpg"></p><p>目录命令:cd /usr/local/software/<br>解压命令:tar -zxvf /usr/local/software/fastdfs-nginx-module_v1.16.tar.gz -C /usr/local/fast/<br>3 进入目录:cd fastdfs-nginx-module/src/<br><img src="/2018/11/10/Fastdfs分布式文件存储/25.jpg" alt="25.jpg"></p><p>4 编辑配置文件config<br>命令: vim /usr/local/fast/fastdfs-nginx-module/src/config<br>修改内容:去掉下图中的local文件层次<br><img src="/2018/11/10/Fastdfs分布式文件存储/26.jpg" alt="26.jpg"></p><p>修改完毕为:<br><img src="/2018/11/10/Fastdfs分布式文件存储/27.jpg" alt="27.jpg"></p><p>5 FastDFS与nginx进行集成<br>首先把之前的nginx进行删除<br>目录命令:cd /usr/local/<br>删除命令:rm -rf nginx<br>进入到nginx目录命令:cd nginx-1.6.2/<br>加入模块命令:./configure --add-module=/usr/local/fast/fastdfs-nginx-module/src/<br>重新编译命令:make &amp;&amp; make install<br>6 复制fastdfs-ngin-module中的配置文件,到/etc/fdfs目录中,如图所示:<br><img src="/2018/11/10/Fastdfs分布式文件存储/28.jpg" alt="28.jpg"></p><p>copy命令:cp /usr/local/fast/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/<br>7 进行修改 /etc/fdfs/ 目录下,我们刚刚copy过来的mod_fastdfs.conf 文件。<img src="/2018/11/10/Fastdfs分布式文件存储/29.jpg" alt="29.jpg"></p><p>命令:vim /etc/fdfs/mod_fastdfs.conf修改内容:比如连接超时时间、跟踪器路径配置、url的group配置、connect_timeout=10tracker_server=192.168.1.172:22122<br>url_have_group_name = true<br>store_path0=/fastdfs/storage<br>8 复制FastDFS里的2个文件,到/etc/fdfs目录中,如图所示:<br><img src="/2018/11/10/Fastdfs分布式文件存储/30.jpg" alt="30.jpg"></p><p>目录命令:cd /usr/local/fast/FastDFS/conf/<br>Copy命令:cp http.conf mime.types /etc/fdfs/<br>9创建一个软连接,在/fastdfs/storage文件存储目录下创建软连接,将其链接到实际存放数据的目录。<br>命令:ln -s /fastdfs/storage/data/ /fastdfs/storage/data/M00<br>10 修改Nginx配置文件,如图所示:<br><img src="/2018/11/10/Fastdfs分布式文件存储/31.jpg" alt="31.jpg"></p><p>命令:vim nginx.conf修改配置内容如下图所示:<br><img src="/2018/11/10/Fastdfs分布式文件存储/32.jpg" alt="32.jpg"></p><p>修改内容为:<br>listen 8888;<br>server_name localhost;location ~/group([0-9])/M00 {<br>#alias /fastdfs/storage/data;<br>ngx_fastdfs_module;<br>}<br>注意:nginx里的端口要和第五步配置FastDFS存储中的storage.conf文件配置一致,也就是(http.server_port=8888)<br>11 最后检查防火墙,然后我们启动nginx服务<br><img src="/2018/11/10/Fastdfs分布式文件存储/33.jpg" alt="33.jpg"></p><p>启动命令:/usr/local/nginx/sbin/nginx,我们刚才上传了一个文件,上传成功,<br>如图:<br><img src="/2018/11/10/Fastdfs分布式文件存储/34.jpg" alt="34.jpg"></p><p>现在我们使用这个ID用浏览器访问地址:http://192.168.1.173:8888/group1/M00/00/00/wKgBrVaSvM6AddWWAAVFOL7FJU4.tar.gz我们就可以下载这个文件啦!如下图所示:<br><img src="/2018/11/10/Fastdfs分布式文件存储/35.jpg" alt="35.jpg"></p><p>运维注意:我们在使用FastDFS的时候,需要正常关机,不要使用kill -9强杀FastDFS进程,不然会在文件上传时出现丢数据的情况。到此,我们的FastDFS与Nginx整合完毕!!<br>八:启动停止服务步骤如下:<br>启动命令:<br>启动tracker命令:/etc/init.d/fdfs_trackerd start<br>查看进程命令:ps -el | grep fdfs<br>启动storage命令:/etc/init.d/fdfs_storaged start<br>查看进程命令:ps -el | grep fdfs<br>启动nginx命令:/usr/local/nginx/sbin/nginx停止命令:<br>停止tracker命令:/etc/init.d/fdfs_trackerd stop<br>关闭storage命令:/etc/init.d/fdfs_storaged  stop<br>关闭nginx命令:/usr/local/nginx/sbin/nginx -s stop</p><h2>集群环境安装</h2><p>集群环境安装我就稍微简略点了，99%是相同的，说下差异点，主要是一个group的区分，单节点tracker时没指定group，默认为group0<br>首先准备几台机器<br>192.168.1.113 track-group1<br>192.168.1.114 track-group2<br>192.168.1.115 storage-group1-1<br>192.168.1.116 storage-group1-2<br>192.168.1.117 storage-group2-1<br>192.168.1.118 storage-group2-2</p><p>创建tracker节点无区别，依葫芦画瓢创建storage节点时需要指明并区分当前storage节点是挂载在哪个tracker下group的概念主要是为了区分不同的数据来源，可能来自不能的系统，不同的应用，当然在文件传输时不指明group会依照nginx的负载均衡算法(轮询，权重等)分配group<br>eg: 115 116为group1  117 118为group2<br>vim storage.confgroup_name = group1(或group2)首先一定要启动tracker，之后启动storage，查看日志<img src="/2018/11/10/Fastdfs分布式文件存储/36.jpg" alt="36.jpg"><br>我们可以发现两个tracker是能互相知道对方的，更能知道当前tracker下挂载了多少个storage，此时storage集群中，拿115 116为例子，一个group中的数据永远是相同的，之间异步进行文件的备份，实现高可用，如果此时115宕机，115会被剔除出group1，恢复后从116进行文件的恢复，并再次加入集群ok,当我们系统中存在多个tracker集群时,此时tracker就不应该直接与客户端通信，在nginx网管层(多层nginx+keeplived虚拟出svip)进行负载均衡</p><h2>java客户端操作</h2><p>此处结合spring＋mybatis＋springmvc来进行操作，ssm的基本配置不需多说</p><ol><li><p>导入依赖,国产的，包里的代码都有详细的注释，一目了然<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.github.tobato&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;fastdfs-client&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.25.4-RELEASE&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></p></li><li><p>添加springmvc中multipartFile的bean配置，此处可以进行文件类型，文件大小，文件编码等设置，不细说，网上一大把<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;bean id=&quot;multipartResolver&quot;</span><br><span class="line">      class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></li><li><p>添加fastdfs客户端连接配置，主要是一些连接器，连接池的配置，也可以进行超时时间，签名验签token校验等，这里给出最进本的配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;</span><br><span class="line">       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;</span><br><span class="line">       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans</span><br><span class="line">       http://www.springframework.org/schema/beans/spring-beans.xsd</span><br><span class="line">       http://www.springframework.org/schema/context</span><br><span class="line">       http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--配置扫描包--&gt;</span><br><span class="line">    &lt;context:component-scan base-package=&quot;com.github.tobato.fastdfs.service,com.github.tobato.fastdfs.domain&quot;/&gt;</span><br><span class="line">    &lt;!--配置连接管理器--&gt;</span><br><span class="line">    &lt;bean id=&quot;trackerConnectionManager&quot; class=&quot;com.github.tobato.fastdfs.conn.TrackerConnectionManager&quot;&gt;</span><br><span class="line">        &lt;constructor-arg name=&quot;pool&quot; ref=&quot;fdfsConnectionPool&quot;&gt;</span><br><span class="line">        &lt;/constructor-arg&gt;</span><br><span class="line">        &lt;!--配置fastDFS tracker 服务器 ip:port 地址--&gt;</span><br><span class="line">        &lt;property name=&quot;trackerList&quot;&gt;</span><br><span class="line">            &lt;list&gt;</span><br><span class="line">                &lt;value&gt;10.211.55.5:22122&lt;/value&gt;</span><br><span class="line">            &lt;/list&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">    &lt;/bean&gt;</span><br><span class="line">    &lt;!--配置连接池--&gt;</span><br><span class="line">    &lt;bean id=&quot;fdfsConnectionPool&quot; class=&quot;com.github.tobato.fastdfs.conn.FdfsConnectionPool&quot;&gt;</span><br><span class="line">        &lt;!--注入连接池配置--&gt;</span><br><span class="line">        &lt;constructor-arg name=&quot;config&quot; &gt;</span><br><span class="line">            &lt;bean class=&quot;com.github.tobato.fastdfs.conn.ConnectionPoolConfig&quot;/&gt;</span><br><span class="line">        &lt;/constructor-arg&gt;</span><br><span class="line">        &lt;!--注入连接池工厂--&gt;</span><br><span class="line">        &lt;constructor-arg name=&quot;factory&quot; &gt;</span><br><span class="line">            &lt;bean class=&quot;com.github.tobato.fastdfs.conn.PooledConnectionFactory&quot;/&gt;</span><br><span class="line">        &lt;/constructor-arg&gt;</span><br><span class="line">    &lt;/bean&gt;</span><br><span class="line">&lt;/beans&gt;</span><br></pre></td></tr></table></figure></p></li></ol><p>4.编写test类demo<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">package org.seckill;</span><br><span class="line"></span><br><span class="line">import com.github.tobato.fastdfs.domain.StorePath;</span><br><span class="line">import com.github.tobato.fastdfs.service.FastFileStorageClient;</span><br><span class="line">import org.junit.Test;</span><br><span class="line">import org.junit.runner.RunWith;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line">import org.springframework.test.context.ContextConfiguration;</span><br><span class="line">import org.springframework.test.context.junit4.AbstractJUnit4SpringContextTests;</span><br><span class="line">import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;</span><br><span class="line"></span><br><span class="line">import javax.annotation.Resource;</span><br><span class="line">import java.io.File;</span><br><span class="line">import java.io.FileInputStream;</span><br><span class="line"></span><br><span class="line">@ContextConfiguration(locations = &#123;&quot;classpath:spring/spring-fastdfs.xml&quot;&#125;)</span><br><span class="line">@RunWith(SpringJUnit4ClassRunner.class)</span><br><span class="line">public class FastDFSDemo extends AbstractJUnit4SpringContextTests &#123;</span><br><span class="line"></span><br><span class="line">    private static final Logger logger = LoggerFactory.getLogger(FastDFSDemo.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Resource</span><br><span class="line">    private FastFileStorageClient fastFileStorageClient;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     *上传</span><br><span class="line">     *</span><br><span class="line">     * @author ❤ xiemao</span><br><span class="line">     * @date : 18/11/01 下午8:14</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void uploadFile() throws Exception &#123;</span><br><span class="line">        File file = new File(&quot;/Users/xie199475/备份/provider/src/main/resources/file_source/001.jpg&quot;);</span><br><span class="line">        StorePath storePath = fastFileStorageClient.</span><br><span class="line">                uploadFile(null, new FileInputStream(file), file.length(), &quot;jpg&quot;);</span><br><span class="line"></span><br><span class="line">        logger.info(&quot;------:&#123;&#125;&quot;,storePath.getFullPath());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     *删除</span><br><span class="line">     *</span><br><span class="line">     * @author ❤ xiemao</span><br><span class="line">     * @date : 18/11/01 下午8:15</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void delete()&#123;</span><br><span class="line">        String path = &quot;group1/M00/00/00/CtM3BlvUxfGAaC-fAAVFOL7FJU4.tar.gz&quot;;</span><br><span class="line">        fastFileStorageClient.deleteFile(path);</span><br><span class="line">        logger.info(&quot;----ok&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@RequestMapping(value = &quot;/upload&quot;, method = RequestMethod.POST)</span><br><span class="line">   public void upload(MultipartFile file, FileUpload fileUpload) throws IOException &#123;</span><br><span class="line">       StorePath storePath = fastFileStorageClient.</span><br><span class="line">               uploadFile(null, new ByteArrayInputStream(file.getBytes()), Long.valueOf(fileUpload.getSize()), &quot;jpg&quot;);</span><br><span class="line"></span><br><span class="line">       logger.info(&quot;------:&#123;&#125;&quot;, storePath.getFullPath());</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   @RequestMapping(value = &quot;/download&quot;, method = RequestMethod.POST)</span><br><span class="line">   public void download(HttpServletResponse response) throws Exception &#123;</span><br><span class="line">       try (OutputStream os = response.getOutputStream();</span><br><span class="line">            BufferedOutputStream bf = new BufferedOutputStream(os)) &#123;</span><br><span class="line">           String groupName = &quot;group1&quot;;</span><br><span class="line">           String path = &quot;M00/00/00/CtM3Blvj9PKAYquVAAD9K7ssbVE777.jpg&quot;;</span><br><span class="line">           byte[] data = fastFileStorageClient.downloadFile(groupName, path, new DownloadByteArray());</span><br><span class="line">           IOUtils.write(data, bf);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p><h2>配置文件参数说明</h2><p>storage.conf<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">storage.conf配置文件分析：</span><br><span class="line"></span><br><span class="line">#同tracker.conf</span><br><span class="line">disabled=false</span><br><span class="line"> </span><br><span class="line">#这个storage服务器属于那个group</span><br><span class="line">group_name=group1</span><br><span class="line"> </span><br><span class="line">#同tracker.conf</span><br><span class="line">bind_addr=</span><br><span class="line"> </span><br><span class="line">#连接其他服务器时是否绑定地址，bind_addr配置时本参数才有效</span><br><span class="line">client_bind=true</span><br><span class="line"> </span><br><span class="line">#同tracker.conf</span><br><span class="line">port=23000</span><br><span class="line">connect_timeout=30</span><br><span class="line">network_timeout=60</span><br><span class="line"> </span><br><span class="line">#主动向tracker发送心跳检测的时间间隔</span><br><span class="line">heart_beat_interval=30</span><br><span class="line"> </span><br><span class="line">#主动向tracker发送磁盘使用率的时间间隔</span><br><span class="line">stat_report_interval=60</span><br><span class="line"> </span><br><span class="line">#同tracker.conf</span><br><span class="line">base_path=/opt/fdfs</span><br><span class="line">max_connections=256</span><br><span class="line"> </span><br><span class="line">#接收/发送数据的buff大小，必须大于8KB</span><br><span class="line">buff_size = 256KB</span><br><span class="line"> </span><br><span class="line">#同tracker.conf</span><br><span class="line">work_threads=4</span><br><span class="line"> </span><br><span class="line">#磁盘IO是否读写分离</span><br><span class="line">disk_rw_separated = true</span><br><span class="line"> </span><br><span class="line">#是否直接读写文件，默认关闭</span><br><span class="line">disk_rw_direct = false</span><br><span class="line"> </span><br><span class="line">#混合读写时的读写线程数</span><br><span class="line">disk_reader_threads = 1</span><br><span class="line">disk_writer_threads = 1</span><br><span class="line"> </span><br><span class="line">#同步文件时如果binlog没有要同步的文件，则延迟多少毫秒后重新读取，0表示不延迟</span><br><span class="line">sync_wait_msec=50</span><br><span class="line"> </span><br><span class="line">#同步完一个文件后间隔多少毫秒同步下一个文件，0表示不休息直接同步</span><br><span class="line">sync_interval=0</span><br><span class="line"> </span><br><span class="line">#表示这段时间内同步文件</span><br><span class="line">sync_start_time=00:00</span><br><span class="line">sync_end_time=23:59</span><br><span class="line"> </span><br><span class="line">#同步完多少文件后写mark标记</span><br><span class="line">write_mark_file_freq=500</span><br><span class="line"> </span><br><span class="line">#storage在存储文件时支持多路径，默认只设置一个</span><br><span class="line">store_path_count=1</span><br><span class="line"> </span><br><span class="line">#配置多个store_path路径，从0开始，如果store_path0不存在，则base_path必须存在</span><br><span class="line">store_path0=/opt/fdfs</span><br><span class="line">#store_path1=/opt/fastdfs2</span><br><span class="line"> </span><br><span class="line">#subdir_count  * subdir_count个目录会在store_path下创建，采用两级存储</span><br><span class="line">subdir_count_per_path=256</span><br><span class="line"> </span><br><span class="line">#设置tracker_server</span><br><span class="line">tracker_server=x.x.x.x:22122</span><br><span class="line"> </span><br><span class="line">#同tracker.conf</span><br><span class="line">log_level=info</span><br><span class="line">run_by_group=</span><br><span class="line">run_by_user=</span><br><span class="line">allow_hosts=*</span><br><span class="line"> </span><br><span class="line">#文件在数据目录下的存放策略，0:轮训 1:随机</span><br><span class="line">file_distribute_path_mode=0</span><br><span class="line"> </span><br><span class="line">#当问及是轮训存放时，一个目录下可存放的文件数目</span><br><span class="line">file_distribute_rotate_count=100</span><br><span class="line"> </span><br><span class="line">#写入多少字节后就开始同步，0表示不同步</span><br><span class="line">fsync_after_written_bytes=0</span><br><span class="line"> </span><br><span class="line">#刷新日志信息到disk的间隔</span><br><span class="line">sync_log_buff_interval=10</span><br><span class="line"> </span><br><span class="line">#同步storage的状态信息到disk的间隔</span><br><span class="line">sync_stat_file_interval=300</span><br><span class="line"> </span><br><span class="line">#线程栈大小</span><br><span class="line">thread_stack_size=512KB</span><br><span class="line"> </span><br><span class="line">#设置文件上传服务器的优先级，值越小越高</span><br><span class="line">upload_priority=10</span><br><span class="line"> </span><br><span class="line">#是否检测文件重复存在，1:检测 0:不检测</span><br><span class="line">check_file_duplicate=0</span><br><span class="line"> </span><br><span class="line">#当check_file_duplicate设置为1时，次值必须设置</span><br><span class="line">key_namespace=FastDFS</span><br><span class="line"> </span><br><span class="line">#与FastDHT建立连接的方式 0:短连接 1:长连接</span><br><span class="line">keep_alive=0</span><br><span class="line"> </span><br><span class="line">#同tracker.conf</span><br><span class="line">http.disabled=false</span><br><span class="line">http.domain_name=</span><br><span class="line">http.server_port=8888</span><br><span class="line">http.trunk_size=256KB</span><br><span class="line">http.need_find_content_type=true</span><br><span class="line">##include http.conf</span><br></pre></td></tr></table></figure></p><p>tracker.conf<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">tracker.conf 配置文件分析：</span><br><span class="line"></span><br><span class="line">#配置tracker.conf这个配置文件是否生效，因为在启动fastdfs服务端进程时需要指定配置文件，所以需要使次配置文件生效。false是生效，true是屏蔽。</span><br><span class="line">disabled=false</span><br><span class="line"> </span><br><span class="line">#程序的监听地址，如果不设定则监听所有地址</span><br><span class="line">bind_addr=</span><br><span class="line"> </span><br><span class="line">#tracker监听的端口</span><br><span class="line">port=22122</span><br><span class="line"> </span><br><span class="line">#链接超时设定</span><br><span class="line">connect_timeout=30</span><br><span class="line"> </span><br><span class="line">#tracker在通过网络发送接收数据的超时时间</span><br><span class="line">network_timeout=60</span><br><span class="line"> </span><br><span class="line">#数据和日志的存放地点</span><br><span class="line">base_path=/opt/fdfs</span><br><span class="line"> </span><br><span class="line">#服务所支持的最大链接数</span><br><span class="line">max_connections=256</span><br><span class="line"> </span><br><span class="line">#工作线程数一般为cpu个数</span><br><span class="line">work_threads=4</span><br><span class="line"> </span><br><span class="line">#在存储文件时选择group的策略，0:轮训策略 1:指定某一个组 2:负载均衡，选择空闲空间最大的group</span><br><span class="line">store_lookup=2</span><br><span class="line"> </span><br><span class="line">#如果上面的store_lookup选择了1，则这里需要指定一个group</span><br><span class="line">#store_group=group2</span><br><span class="line"> </span><br><span class="line">#在group中的哪台storage做主storage，当一个文件上传到主storage后，就由这台机器同步文件到group内的其他storage上，0：轮训策略 1：根据ip地址排序，第一个 2:根据优先级排序，第一个</span><br><span class="line">store_server=0</span><br><span class="line"> </span><br><span class="line">#选择那个storage作为主下载服务器，0:轮训策略 1:主上传storage作为主下载服务器</span><br><span class="line">download_server=0</span><br><span class="line"> </span><br><span class="line">#选择文件上传到storage中的哪个(目录/挂载点),storage可以有多个存放文件的base path 0:轮训策略 2:负载均衡，选择空闲空间最大的</span><br><span class="line">store_path=0</span><br><span class="line"> </span><br><span class="line">#系统预留空间，当一个group中的任何storage的剩余空间小于定义的值，整个group就不能上传文件了</span><br><span class="line">reserved_storage_space = 4GB</span><br><span class="line"> </span><br><span class="line">#日志信息级别</span><br><span class="line">log_level=info</span><br><span class="line"> </span><br><span class="line">#进程以那个用户/用户组运行，不指定默认是当前用户</span><br><span class="line">run_by_group=</span><br><span class="line">run_by_user=</span><br><span class="line"> </span><br><span class="line">#允许那些机器连接tracker默认是所有机器</span><br><span class="line">allow_hosts=*</span><br><span class="line"> </span><br><span class="line">#设置日志信息刷新到disk的频率，默认10s</span><br><span class="line">sync_log_buff_interval = 10</span><br><span class="line"> </span><br><span class="line">#检测storage服务器的间隔时间，storage定期主动向tracker发送心跳，如果在指定的时间没收到信号，tracker人为storage故障，默认120s</span><br><span class="line">check_active_interval = 120</span><br><span class="line"> </span><br><span class="line">#线程栈的大小，最小64K</span><br><span class="line">thread_stack_size = 64KB</span><br><span class="line"> </span><br><span class="line">#storage的ip改变后服务端是否自动调整，storage进程重启时才自动调整</span><br><span class="line">storage_ip_changed_auto_adjust = true</span><br><span class="line"> </span><br><span class="line">#storage之间同步文件的最大延迟，默认1天</span><br><span class="line">storage_sync_file_max_delay = 86400</span><br><span class="line"> </span><br><span class="line">#同步一个文件所花费的最大时间</span><br><span class="line">storage_sync_file_max_time = 300</span><br><span class="line"> </span><br><span class="line">#是否用一个trunk文件存储多个小文件</span><br><span class="line">use_trunk_file = false</span><br><span class="line"> </span><br><span class="line">#最小的solt大小，应该小于4KB，默认256bytes</span><br><span class="line">slot_min_size = 256</span><br><span class="line"> </span><br><span class="line">#最大的solt大小，如果上传的文件小于默认值，则上传文件被放入trunk文件中</span><br><span class="line">slot_max_size = 16MB</span><br><span class="line"> </span><br><span class="line">#trunk文件的默认大小，应该大于4M</span><br><span class="line">trunk_file_size = 64MB</span><br><span class="line"> </span><br><span class="line">#http服务是否生效，默认不生效</span><br><span class="line">http.disabled=false</span><br><span class="line"> </span><br><span class="line">#http服务端口</span><br><span class="line">http.server_port=8080</span><br><span class="line"> </span><br><span class="line">#检测storage上http服务的时间间隔，&lt;=0表示不检测</span><br><span class="line">http.check_alive_interval=30</span><br><span class="line"> </span><br><span class="line">#检测storage上http服务时所用请求的类型，tcp只检测是否可以连接，http必须返回200</span><br><span class="line">http.check_alive_type=tcp</span><br><span class="line"> </span><br><span class="line">#通过url检测storage http服务状态</span><br><span class="line">http.check_alive_uri=/status.html</span><br><span class="line"> </span><br><span class="line">#if need find content type from file extension name</span><br><span class="line">http.need_find_content_type=true</span><br><span class="line"> </span><br><span class="line">#用include包含进http的其他设置</span><br><span class="line">##include http.conf</span><br></pre></td></tr></table></figure></p><p>over :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;Fastdfs分布式文件存储&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;     Fastdfs是一款比较轻量级的开源分布式文件系统，作者是余庆大大在0几年业余时间用c撸出来的(工作不饱和，加班不够啊:)  ，现任淘宝网开放平台技术部资深架构师,Fastdfs在小
      
    
    </summary>
    
      <category term="分布式" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="fastdfs" scheme="http://yoursite.com/tags/fastdfs/"/>
    
      <category term="文件存储" scheme="http://yoursite.com/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
</feed>
