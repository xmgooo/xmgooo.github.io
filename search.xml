<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[redis总结]]></title>
    <url>%2F2018%2F11%2F24%2Fredis%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[关于redis的一点总结 引言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;常规的应用系统的系统瓶颈大部分都出现在数据库的IO操作，并且90%的请求基本都是select操作，对于数据库的优化手段可以有非常多的手段，举几种常见的:1.最基本的数据库表结构设计优化，理论上的三大范式，表结构结合业务设计的合理性，是否可从业务角度优化，如两次查询变为一次查询，联表查变为单表操作，跨库操作是否可以避免或者优化，sql语句的优化等。2.索引，代价最小，实现难度最低，效果提升显著，但要考虑索引维护带来的性能开销，对于一些批量更新或者集中的update，delete操作，可以考虑动态索引，即在开启操作前删除索引，执行完后再添加上索引。3.mysql参数的优化，连接数，缓冲池大小等。4.读写分离，扩展slave节点，分担读请求的压力，但要注意数据同步操作可能带来的延迟性问题。5.大表拆分，这里可分为水平和垂直两个方向。水平拆分，分表，减少单表数据量过大的问题，一般mysql单表数据控制在1000w性能最优，注意拆分业务的合理性，常用的可以有主键hash，业务字段取余（如日期，订单），但要提前规划好容量。垂直拆分，1次查询变为两次关联，冷热数据字段分离等。6.分库，应对单库性能瓶颈的一种手段，但要注意可能带来的跨库操作，或者复杂查询（count，group by，top）等增加的复杂性。7.缓存。 缓存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于缓存，缓存分为很多种，页面缓存cookie，基于会话，session的临时缓存springcache，hibernate，mybatis等，基于内存的redis/memcached等缓存，常用的redis和memcached，memcached没研究过，不过多说了，网上对比的资料也很多，感觉能用memcached的地方都能用redis代替，并且memcached不支持持久化操作和事务，这点限定了它的很多使用场景，至于它的在小数据量下相比于redis的性能优点，redis通过集群的手段也能有所弥补，本身已经够快了，那点速度的差别也微不足道了。在请求量特别大的时候，可以考虑二级缓存，一级缓存通过springcache在进程内缓存一个userid+ip+时间戳等，过滤重复请求或者业务处理，不过这种缓存是存在某一台应用服务器的，可能需要将ng等负载均衡服务器的策略改为定向转发而不是轮询等，二级缓存再考虑redis等。 redis特点1.非关系型数据库（数据模型简单）2.基于内存，基于内存优于基于磁盘本身的特色(ps个笑话：因为内存距离cpu的距离比硬盘更近，所以性能好:) ,读写能力高，Redis能读的速度可达110000次/s,写的速度是81000次/s，具体可参考官网。3.单线程，和多线程相比的优势，避免的线程的上下文切换和锁竞争问题，加锁释放锁等资源消耗，死锁等一系列问题都可以被屏蔽掉，单线程的特色决定了cpu本身就不可能成为性能瓶颈，但是在应对多核cpu的处理器，单线程就可能显的比较鸡肋和奢侈了，这种的应对办法一般是在单机上部署多个redis实例，比如8核的cpu，理论上可以部署8个redis实例，但是这也只是想象的，因为本身redis的持久化操作和数据同步等操作都会开启临时子线程去处理，如果部署太多实例，实例间的通信和同步带来的开销可能反而会使性能降低，所以需综合cpu的实际情况进行考量，真正决定redis性能的在于实际内存大小和网络io。网络io分为客户端的请求io和节点间同步或通信的io，客户端请求io操作优化方向无非两点，次数和大小，批量批量提交，较少io次数，序列化请求数据，protobuf，kyro等，节点间同步或通信io操作建议只在slave节点aof持久化操作，master节点不进行持久化，master节点的持久化操作可能带来写入性能的不稳定。对于节点间的同步io消耗，如果将master和slave部署在同一台机器上，确实会极大的降低io，但是不建议这么做，还是建议采取交叉部署的方式，将同一个分区的master和slave分散在不同机器。 关于内存的使用，建议不要让你的 Redis 所在机器物理内存使用超过实际内存总量的3/5，详细分析可以参考下https://mp.weixin.qq.com/s/5TtTplThal2otR8xJB6OEw 持久化操作：rdb和aof&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rbd：周期性的刷盘，相当于linux中的快照概念，所以也被称为快照持久化，如每一秒定时将内存中的数据写入硬盘中，不适用于实时的系统，如想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。save命令和bgsave命令都可以生成RDB文件，线上环境应该禁掉save命令，这是个同步操作，会阻塞主进程，而bgsave命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;aof：类似于mysql的binlog，和oracle中的undo，实时将每次执行的写命令保存到硬盘，宕机或者掉电后数据可恢复，但实时的日志会带来额外的性能开销。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;两种方式的取舍主要是在持久化的性能和一致性之间做取舍。给出自己的一点理解吧：1.晚上执行定时任务跑批量或者缓存更新备份或者数据恢复的时候会用rdb，跑批失败了可以恢复到跑批前的快照，重新执行定时任务或者重新备份即可。2.并发量不大情况下，对数据完整性要求较高时，可启用aof。 aof应该3.若只打算用Redis 做缓存，可以关闭持久化，仅靠master／salve replication实现高可用也行，能节省一大笔io,代价是如果master／salve同时宕机（此时建议采用交叉部署，master部署在a机房，slave1部署在b机房，slave2部署在c机房，虽然跨机房的部署同步会增加io，但相比于aof的io就显得微不足道了，这种可以将一个集群同时宕机的风险降到最低，只要一个节点活着，数据即是完整，除非你说北京，上海，广州的机房同时炸了。）4.关于rdb和aof更多详情信息，可以参考下这篇https://mp.weixin.qq.com/s/XmFygIm5zuR8ni5ak91-uQ 版本比较&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.6版本推出了哨兵的机制，但是特别不稳定，各种各样的bug，2.8版本这个功能才算稳定下来，2.x哨兵的作用是一是监控主从，定时的心跳检测包看集群是否健康，二是实现切换，master宕机后通过选举算法在此master下的slave中找一个顶上来。可以配置多个master做高可用，master与master实现复制同步。即使使用哨兵，redis每个实例也是全量存储，每个redis存储的内容都是完整的数据，浪费内存，单点容量上限无法突破。这里比较知名的企业或者组织就衍生开源了一系列方案，为了做数据分片，大致两种思路，分为客户端分片和服务端分片，客户端分片通过代码在客户端实现，但是存在可扩展性差，不能跨语言，不通用等问题，服务端分片都是加一层proxy代理层集群加zk来做的，通过proxy代理层重定向到各个集群分片，实现难度较大，增加了系统复杂度（一个很坑和很冷的笑话：没有什么架构问题是向上加一层不能解决的，如果不能，那就加两层）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.x版本较2.x版本的master/slave+哨兵的架构上有了很大扩展，抛弃了之前的架构，基于分片进行存储，采用hash进行分区映射，解决了以前master/slave+哨兵模式中的内存/QPS受限于单机的困境，即每台master redis存储不同的内容，master节点上具备slot槽的概念，存储写入时通过hash确定写入到哪个节点，读取时也是先确定数据在哪个槽上，再找到对应的节点，之后在本机上进行读取，这里要说明下，每个master节点下可挂载多个slave节点，slave节点本身是不具备槽的。有时为了解决master下挂载过多slave数据同步带来的延迟问题，可能会采用master下只挂在一个slave1，slave1下再挂在slave2，slave2下再挂在slave3这种串行的架构，这种模式其实我没明白，如果slave1断开的话，后面的一串跟master怎么通信（有待考核）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;集群的这种架构去中心化，内联通信，本身master节点又可以存储数据，又可以与其它master进行通信，可用于大数据量高可用Cache/存储等场景。这种内联的架构感觉更符合现代互联网的架构模式，将监控与应用结合在一起，每个节点即是执行者也是监督者，节点之间互相通信，底层实现的话目前还不太清楚，节点之间的依赖问题，通信带来的延迟问题感觉会有特殊的处理手段，目前想不到怎么做的两种不同的代表：Elasticsearch类比3.x和dubbo+zk类比2.x。 架构演进图单节点slave节点分担读压力2.6版本后加入哨兵集群，实现主从切换2.x版本通过proxy实现分片存储，减少master写压力3x版本集群 单实例安装下载地址http://redis.io/download安装步骤：1 首先需要安装gcc，把下载好的redis-3.0.0-rc2.tar.gz 放到linux /usr/local文件夹下2 进行解压 tar -zxvf redis-3.0.0-rc2.tar.gz3 进入到redis-3.0.0目录下，进行编译 make4 进入到src下进行安装 make install 验证(ll查看src下的目录，有redis-server 、redis-cil即可)5 建立俩个文件夹存放redis命令和配置文件mkdir -p /usr/local/redis/etcmkdir -p /usr/local/redis/bin6 把redis-3.0.0下的redis.conf 移动到/usr/local/redis/etc下， cp redis.conf /usr/local/redis/etc/7 把redis-3.0.0/src里的mkreleasehdr.sh、redis-benchmark、redis-check-aof、redis-check-dump、redis-cli、redis-server文件移动到bin下，命令：mv mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-dump redis-cli redis-server /usr/local/redis/bin8 启动时并指定配置文件：/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf（注意要使用后台启动，所以修改redis.conf里的 daemonize 改为yes)9 验证启动是否成功： ps -ef | grep redis 查看是否有redis服务 或者 查看端口：netstat -tunpl | grep 6379进入redis客户端 ./redis-cli 退出客户端quit退出redis服务：（1）pkill redis-server 、（2）kill 进程号、（3）/usr/local/redis/bin/redis-cli shutdown 哨兵模式的安装（1）copy文件sentinel.conf到usr/local/redis/etc中 (2) 修改sentinel.conf文件: sentinel monitor mymaster 192.168.1.174 6379 1 #名称，ip，端口，投票选举次数 sentinel down-after-milliseconds mymaster 5000 #默认1s检测一次，这里配置超时5000毫秒为宕机 sentinel failover-timeout mymaster 9000000 sentinel can-failover mymaster yes sentinel parallel-syncs mymaster 2 (3) 启动sentinel哨兵 /usr/local/redis/bin/redis-server /usr/local/redis/etc/sentinel.conf –sentinel &amp; (4) 查看哨兵相关信息命令 /usr/local/redis/bin/redis-cli -h 192.168.1.175 -p 26379 info sentinel (5) 关闭主服务器 /usr/local/redis/bin/redis-cli -h 192.168.1.175 -p 6379 shutdown Redis 3.0 集群搭建/* Redis 3.0 集群搭建 **/在redis3.0以前，提供了Sentinel工具来监控各Master的状态;如果Master异常，则会做主从切换，将slave作为master，将master作为slave。其配置也是稍微的复杂，并且各方面表现一般。现在redis3.0已经支持集群的容错功能，并且非常简单。下面我们来进行学习下redis3.0如何搭建集群（集群搭建：至少要三个master）。第一步：创建一个文件夹redis-cluster，然后在其下面分别创建6个文件夹如下：（1）mkdir -p /usr/local/redis-cluster（2）mkdir 7001、mkdir 7002、mkdir 7003、mkdir 7004、mkdir 7005、mkdir 7006 第二步：把之前的redis.conf配置文件分别copy到700下，进行修改各个文件内容，也就是对700下的每一个copy的redis.conf文件进行修改！如下：（1）daemonize yes（2）port 700（分别对每个机器的端口号进行设置）（3）bind 192.168.1.171（必须要绑定当前机器的ip，不然会无限悲剧下去哇..深坑勿入！！！）（4）dir /usr/local/redis-cluster/700/（指定数据文件存放位置，必须要指定不同的目录位置，不然会丢失数据，深坑勿入！！！）（5）cluster-enabled yes（启动集群模式，开始玩耍）（6）cluster-config-file nodes700*.conf（这里700x最好和port对应上）（7）cluster-node-timeout 5000（8）append only yes数据文件dump.rdb放入到etc目录下 第三步：注意每个文件要修改端口号，bind的ip，数据存放的dir，并且nodes文件都需要进行修改！ 第四步：由于redis集群需要使用ruby命令，所以我们需要安装ruby（1）yum install ruby（2）yum install rubygems（3）gem install redis （安装redis和ruby的接口） 第五步：分别启动6个redis实例，然后检查是否启动成功（1）/usr/local/redis/bin/redis-server /usr/local/redis-cluster/700*/redis.conf（2）ps -el | grep redis 查看是否启动成功 第六步：首先到redis3.0的安装目录下，然后执行redis-trib.rb命令。（1）cd /usr/local/redis3.0/src（2）./redis-trib.rb create –replicas 1 10.211.55.6:7001 10.211.55.6:7002 10.211.55.6:7003 10.211.55.6:7004 10.211.55.6:7005 10.211.55.6:7006 第七步：到此为止我们集群搭建成功！进行验证：（1）连接任意一个客户端即可：./redis-cli -c -h -p （-c表示集群模式，指定ip地址和端口号）如：/usr/local/redis/bin/redis-cli -c -h 10.211.55.6 -p 7001（2）进行验证：cluster info（查看集群信息）、cluster nodes（查看节点列表）上图集群节点信息说明一下，从左到右分别为 ip:port … 1.id: 节点ID,是一个40字节的随机字符串，这个值在节点启动的时候创建，并且永远不会改变（除非使用CLUSTER RESET HARD命令）。 2.ip:port: 客户端与节点通信使用的地址. 3.flags: 逗号分割的标记位，可能的值有: myself, master, slave, fail?, fail, handshake, noaddr, noflags. 下一部分将详细介绍这些标记. 4.master: 如果节点是slave，并且已知master节点，则这里列出master节点ID,否则的话这里列出”-“。 5.ping-sent: 最近一次发送ping的时间，这个时间是一个unix毫秒时间戳，0代表没有发送过. 6.pong-recv: 最近一次收到pong的时间，使用unix时间戳表示. 7.config-epoch: 节点的epoch值（or of the current master if the node is a slave）。每当节点发生失败切换时，都会创建一个新的，独特的，递增的epoch。如果多个节点竞争同一个哈希槽时，epoch值更高的节点会抢夺到。 8.link-state: node-to-node集群总线使用的链接的状态，我们使用这个链接与集群中其他节点进行通信.值可以是 connected 和 disconnected. 9.slot: 哈希槽值或者一个哈希槽范围. 从第9个参数开始，后面最多可能有16384个 数(limit never reached)。代表当前节点可以提供服务的所有哈希槽值。如果只是一个值,那就是只有一个槽会被使用。如果是一个范围，这个值表示为起始槽-结束槽，节点将处理包括起始槽和结束槽在内的所有哈希槽。 各flags的含义 (上面所说数据项3):myself: 当前连接的节点.master: 节点是master.slave: 节点是slave.fail?: 节点处于PFAIL 状态。 当前节点无法联系，但逻辑上是可达的 (非 FAIL 状态).fail: 节点处于FAIL 状态. 大部分节点都无法与其取得联系将会将改节点由 PFAIL 状态升级至FAIL状态。handshake: 还未取得信任的节点，当前正在与其进行握手.noaddr: 没有地址的节点（No address known for this node）.noflags: 连个标记都没有（No flags at all）.因为这里没有借助第三方插件的管控台，所以看懂节点的状态信息还是非常重要的，结合日志文件的说明可以更加清楚此时节点的健康状态（3）进行数据操作验证（4）关闭集群则需要逐个进行关闭，试验一下主从切换，此时7004为一个master节点，使用命令：/usr/local/redis/bin/redis-cli -c -h 10.211.55.6 -p 7004 shutdown 停掉7004主节点后：注意一下变化7004目前时fail状态，原本是slave的7001此时变为了master，原本7004之上的slot 66-5460移动到了7001，下面是切换的日志 重启7004后，7004重新以slave节点的状态加入集群，并被挂载到7001主节点下，同时开启子线程恢复宕机期间的数据,看下图cluster nodes和日志输出第八步：（补充）友情提示：当出现集群无法启动时，删除临时的数据文件，再次重新启动每一个redis服务，然后重新构造集群环境。 第九步：（集群操作文章）redis-trib.rb官方群操作命令: http://redis.io/topics/cluster-tutorial推荐博客: http://blog.51yip.com/nosql/1726.html/comment-page-1 I/O 多路复用 异步 类型于nginx的epoll 负载均衡的实现。核心的理念了，不充分了解的话感觉对redis还是只停留在应用层面，目前还不是很懂，可以参考下https://blog.csdn.net/happy_wu/article/details/80052617 事务在Redis采用了线程封闭的方式，把任务封闭在一个线程，自然避免了线程安全问题，不过对于需要依赖多个redis操作的复合操作来说，依然可能需要事务,它是是一组命令的集合。Redis事务的实现需要用到 MULTI 和 EXEC 两个命令，事务开始的时候先向Redis服务器发送 MULTI 命令，然后依次发送需要在本次事务中处理的命令，最后再发送 EXEC 命令表示事务命令结束 123456789101112131415161718192021222324252627282930313233 &lt;property name=&quot;enableTransactionSupport&quot; value=&quot;true&quot;&gt; public void useTransaction(boolean use) &#123; redisTemplate.setEnableTransactionSupport(use); &#125; /** * 开启事务 一个begin只能对应一个commit或者rollback，用完即销毁 */ public void beginTransaction() &#123; redisTemplate.multi(); &#125; /** * 同在管控台操作不通，管控台开启multi，数据进入QUEUED阶段，必须执行exec才会刷盘成功 * 客户端连接时，不进行exec，会默认提交所有开启multi的数据 */ public void commit() &#123; redisTemplate.exec(); &#125; public void rollback() &#123; redisTemplate.discard(); &#125; /** * 在finally中记得释放资源，不要执行redisTemplate.exec(); * 如果事务异常进入catch中，catch中的discard会消耗掉multi，此时exec会找不到对应的multi，报错，事务失败 */ public void close() &#123;// redisTemplate.exec(); RedisConnectionUtils.unbindConnection(redisTemplate.getConnectionFactory()); &#125; test： 1234567891011121314151617public void test() &#123; try &#123; redisUtil.useTransaction(true); redisUtil.beginTransaction(); redisUtil.set(&quot;name1&quot;, &quot;xiemao1111&quot;);// redisUtil.commit();// int a = 1 / 0; redisUtil.set(&quot;name2&quot;, &quot;xiemao2222&quot;); redisUtil.commit(); &#125; catch (Exception e) &#123; redisUtil.rollback(); logger.error(&quot;error:&quot;,e); &#125; finally &#123; redisUtil.close(); &#125; &#125; 缓存淘汰策略：redis.conf中的 maxmemory-policy volatile-lru1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。推荐使用。3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。应该也没人用吧，你不删最少使用Key,去随机删。4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。不推荐。5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。依然不推荐。6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐。 实际应用场景，自己的一点体会：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.应用服务器做到无状态：session存储在redis中，做单点登录的时候，就是用hash存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.mysql数据的缓存，接口数据的缓存，最常用也最为常见的功能，有各种各样的选择方式，举两个例子，一种是以mysql的某个字段或者几个字段组合项作为key，对应的数据列存个json，这种是以mysql的为基准来进行缓存的，对于查询service这种天然幂等性的接口，service里面包含了a＋b＋c等n个查询，这种就不建议缓存各个查询条件和结果了，直接service方法的全路径名加＋请求参数拼接的字符串作为key，返回值作为value，走到这个服务的代码如果入参相同，直接走缓存，服务下面的逻辑跳过，service服务等包含更新操作，说到幂等性，对于分布式场景，接口一方面要提供超时机制，一方面要保证接口的可重入性，也就是保证接口的幂等性，可以通过mysql的唯一索引重复插入抛出异常进行回滚控制，也可以借助redis生成唯一标示，标示接口被调用过，防止超时后的重复请求问题。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.消息队列，不建议，毕竟不是专业的，数据不丢失（虽然有事务机制，但做到像mq那样保证生产者的事务回滚机制，消费者的去阻塞堆积的功能，广播消息的实现还是比较复杂的）业务数据有专业的mq，流式数据有kafka等，没必要用redis。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.全局唯一标志，订单号，流水号等需唯一的数据，redis可充当发号器的角色，如在代码中生成一个订单号，保证全局唯一,这时可用redis的setnx，前缀为业务标示加上用户id，如”xxxdb”+”uid”+当前时间戳通过setnx操作，返回值进行判断，如果为1，让入成功，不为1，让入失败，while循环着。记得设置一天的过期时间。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.全局的数据，全局的单例模式的概念只是在当前jvm下的单例，生成分布式环境下唯一资源对象或者是对统一资源进行并发的修改更新操作等，如创建一个如秒杀商品扣库存等操作。1234567891011long num = get count;if(num&lt;1)&#123; logger.error(&quot;库存不足&quot;); return;&#125;long result = decr count;if(result &gt;=0)&#123; //sucs,mysql扣库存&#125;else&#123; result = incr result;//购买失败,redis补偿&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.分布式锁，实现方式很多种，如： (1)数据库的版本号，加个version字段用乐观锁的方式。 (2)zk的临时节点，进来的时候判断下有没有临时节点，没有创建一个获得执行权，执行完删除临时节点，有的话说明资源被抢占，阻塞等待。 (3)redis实现分布式锁，其实有各种各样的方式，我目前理解的好像还没有绝对安全的方式，但是有几种比较典型的错误方式如破坏setnx的原子性操作，客户端时钟与服务器时钟不同步的问题，集群的子线程io异步同步数据在不稳定的情况下可能导致的重复锁的等等问题，可以参考下https://mp.weixin.qq.com/s/8FYMUpaBcgOZ9lEqt-0PKg 这篇文章，写得还可以，对于java两种客户端jedis和stringRedisTemplate，我之前一直都是这么使用的： 12345678public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) &#123; String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false; &#125;&#125; 释放锁应该在finally里面进行释放，避免代码异常索得不到释放12stringRedisTemplate.opsForValue().setIfAbsent(redisKey, startTime.toString());stringRedisTemplate.delete(redisKey); &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.缓存时间的设置，缓存雪崩：即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常处理：参考方案一：一个批量操作，设置expire time时，在expire time的基础上给缓存的失效时间加上一个随机值（1到5分钟），避免集体失效。参考方案二：双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I 从缓存A读数据库，有则直接返回&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;II A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;III 更新线程同时更新缓存A和缓存B。还是推荐1吧，2需要维护两份缓存，麻烦&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.缓存穿透 （1）布隆过滤器：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力 （2）如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9.在cluster模式下，对mset mget命令限制很多，要求批量设置的key 都在同一台redis实例上，否则报异常 解决：不建议用mset等，用hashmap转为json存储，hashmap中的元素增加变为json的覆盖。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10.绝大多数命令的时间复杂度都为O(1) ,对于keys * 这样n时间复杂度的操作在生产环境也应该被禁止掉，执行这一的操作因数据的大小而时间不可控，会造成线程的阻塞，大量请求被堆积。 配置文件参考：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522# Redis 配置文件# 当配置中需要配置内存大小时，可以使用 1k, 5GB, 4M 等类似的格式，其转换方式如下(不区分大小写)## 1k =&gt;1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt;1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024bytes## 内存配置大小写是一样的.比如 1gb 1Gb 1GB 1gB# daemonize no 默认情况下，redis不是在后台运行的，如果需要在后台运行，把该项的值更改为yesdaemonizeyes# 当redis在后台运行的时候，Redis默认会把pid文件放在/var/run/redis.pid，你可以配置到其他地址。#当运行多个redis服务时，需要指定不同的pid文件和端口pidfile /var/run/redis.pid# 指定redis运行的端口，默认是6379port 6379# 指定redis只接收来自于该IP地址的请求，如果不进行设置，那么将处理所有请求，# 在生产环境中最好设置该项# bind127.0.0.1# Specify the path for the unix socket that will be used to listen for#incoming connections. There is no default, so Redis will not listen# on aunix socket when not specified.## unixsocket /tmp/redis.sock#unixsocketperm 755# 设置客户端连接时的超时时间，单位为秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接# 0是关闭此设置timeout0# 指定日志记录级别# Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose#debug 记录很多信息，用于开发和测试# varbose 有用的信息，不像debug会记录那么多#notice 普通的verbose，常用于生产环境# warning 只有非常重要或者严重的信息会记录到日志logleveldebug# 配置log文件地址# 默认值为stdout，标准输出，若后台模式会输出到/dev/null#logfilestdoutlogfile /var/log/redis/redis.log# To enable logging to the system logger, just set &apos;syslog-enabled&apos; toyes,# and optionally update the other syslog parameters to suit yourneeds.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.#syslog-facility local0# 可用数据库数# 默认值为16，默认数据库为0，数据库范围在0-（database-1）之间databases 16################################ 快照 ################################### 保存数据到磁盘，格式如下:## save&lt;seconds&gt; &lt;changes&gt;## 指出在多长时间内，有多少次更新操作，就将数据同步到数据文件rdb。# 相当于条件触发抓取快照，这个可以多个条件配合# # 比如默认配置文件中的设置，就设置了三个条件## save 900 1 900秒内至少有1个key被改变# save 30010 300秒内至少有300个key被改变# save 60 10000 60秒内至少有10000个key被改变save 900 1save 300 10save 60 10000# 存储至本地数据库时（持久化到rdb文件）是否压缩数据，默认为yesrdbcompression yes# 本地持久化数据库文件名，默认值为dump.rdbdbfilename dump.rdb# 工作目录## 数据库镜像备份的文件放置的路径。#这里的路径跟文件名要分开配置是因为redis在进行备份时，先会将当前数据库的状态写入到一个临时文件中，等备份完成时，#再把该该临时文件替换为上面所指定的文件，而这里的临时文件和上面所配置的备份文件都会放在这个指定的路径当中。##AOF文件也会存放在这个目录下面## 注意这里必须制定一个目录而不是文件dir ./################################# 复制################################## 主从复制. 设置该数据库为其他数据库的从数据库.#设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步## slaveof&lt;masterip&gt; &lt;masterport&gt;# 当master服务设置了密码保护时(用requirepass制定的密码)# slav服务连接master的密码##masterauth &lt;master-password&gt;# 当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：## 1)如果slave-serve-stale-data设置为yes(默认设置)，从库会继续相应客户端的请求## 2)如果slave-serve-stale-data是指为no，出去INFO和SLAVOF命令之外的任何请求都会返回一个# 错误&quot;SYNC withmaster in progress&quot;#slave-serve-stale-data yes# 从库会按照一个时间间隔向主库发送PINGs.可以通过repl-ping-slave-period设置这个时间间隔，默认是10秒##repl-ping-slave-period 10# repl-timeout 设置主库批量数据传输时间或者ping回复时间间隔，默认值是60秒#一定要确保repl-timeout大于repl-ping-slave-period# repl-timeout 60################################## 安全#################################### 设置客户端连接后进行任何其他指定前需要使用的密码。#警告：因为redis速度相当快，所以在一台比较好的服务器下，一个外部的用户可以在一秒钟进行150K次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解##requirepass foobared# 命令重命名.## 在一个共享环境下可以重命名相对危险的命令。比如把CONFIG重名为一个不容易猜测的字符。##举例:## rename-command CONFIGb840fc02d524045429941cc15f59e41cb7be6c52##如果想删除一个命令，直接把它重命名为一个空字符&quot;&quot;即可，如下：## rename-command CONFIG &quot;&quot;################################### 约束##################################### 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，# 如果设置maxclients 0，表示不作限制。# 当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clientsreached错误信息## maxclients 128# 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key#Redis同时也会移除空的list对象##当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作##注意：Redis新的vm机制，会把Key存放内存，Value会存放在swap区##maxmemory的设置比较适合于把redis当作于类似memcached的缓存来使用，而不适合当做一个真实的DB。#当把Redis当做一个真实的数据库使用的时候，内存使用将是一个很大的开销# maxmemory &lt;bytes&gt;# 当内存达到最大值的时候Redis会选择删除哪些数据？有五种方式可供选择## volatile-lru -&gt;利用LRU算法移除设置过过期时间的key (LRU:最近使用 Least Recently Used )# allkeys-lru -&gt;利用LRU算法移除任何key# volatile-random -&gt; 移除设置过过期时间的随机key#allkeys-&gt;random -&gt; remove a random key, any key# volatile-ttl -&gt;移除即将过期的key(minor TTL)# noeviction -&gt; 不移除任何可以，只是返回一个写错误##注意：对于上面的策略，如果没有合适的key可以移除，当写的时候Redis会返回一个错误## 写命令包括: set setnxsetex append# incr decr rpush lpush rpushx lpushx linsert lsetrpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstorezadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrbydecrby# getset mset msetnx exec sort## 默认是:##maxmemory-policy volatile-lru# LRU 和 minimal TTL 算法都不是精准的算法，但是相对精确的算法(为了节省内存)，随意你可以选择样本大小进行检测。#Redis默认的灰选择3个样本进行检测，你可以通过maxmemory-samples进行设置## maxmemory-samples3############################## AOF ################################默认情况下，redis会在后台异步的把数据库镜像备份到磁盘，但是该备份是非常耗时的，而且备份也不能很频繁，如果发生诸如拉闸限电、拔插头等状况，那么将造成比较大范围的数据丢失。#所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式。# 开启appendonly模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态。#但是这样会造成appendonly.aof文件过大，所以redis还支持了BGREWRITEAOF指令，对appendonly.aof 进行重新整理。#你可以同时开启asynchronous dumps 和 AOFappendonly no# AOF文件名称 (默认: &quot;appendonly.aof&quot;)# appendfilename appendonly.aof# Redis支持三种同步AOF文件的策略:## no: 不进行同步，系统去操作 . Faster.# always:always表示每次有写操作都进行同步. Slow, Safest.# everysec: 表示对写操作进行累积，每秒同步一次.Compromise.## 默认是&quot;everysec&quot;，按照速度和安全折中这是最好的。#如果想让Redis能更高效的运行，你也可以设置为&quot;no&quot;，让操作系统决定什么时候去执行#或者相反想让数据更安全你也可以设置为&quot;always&quot;## 如果不确定就用 &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# AOF策略设置为always或者everysec时，后台处理进程(后台保存或者AOF日志重写)会执行大量的I/O操作#在某些Linux配置中会阻止过长的fsync()请求。注意现在没有任何修复，即使fsync在另外一个线程进行处理##为了减缓这个问题，可以设置下面这个参数no-appendfsync-on-rewrite## This means that whileanother child is saving the durability of Redis is# the same as &quot;appendfsyncnone&quot;, that in pratical terms means that it is# possible to lost up to 30seconds of log in the worst scenario (with the# default Linuxsettings).## If you have latency problems turn this to &quot;yes&quot;. Otherwiseleave it as# &quot;no&quot; that is the safest pick from the point of view ofdurability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# AOF 自动重写#当AOF文件增长到一定大小的时候Redis能够调用 BGREWRITEAOF 对日志文件进行重写##它是这样工作的：Redis会记住上次进行些日志后文件的大小(如果从开机以来还没进行过重写，那日子大小在开机的时候确定)##基础大小会同现在的大小进行比较。如果现在的大小比基础大小大制定的百分比，重写功能将启动#同时需要指定一个最小大小用于AOF重写，这个用于阻止即使文件很小但是增长幅度很大也去重写AOF文件的情况# 设置 percentage为0就关闭这个特性auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb################################## SLOW LOG#################################### Redis Slow Log 记录超过特定执行时间的命令。执行时间不包括I/O计算比如连接客户端，返回结果等，只是命令执行时间##可以通过两个参数设置slow log：一个是告诉Redis执行超过多少时间被记录的参数slowlog-log-slower-than(微妙)，#另一个是slow log 的长度。当一个新命令被记录的时候最早的命令将被从队列中移除# 下面的时间以微妙微单位，因此1000000代表一分钟。#注意制定一个负数将关闭慢日志，而设置为0将强制每个命令都会记录slowlog-log-slower-than 10000# 对日志长度没有限制，只是要注意它会消耗内存# 可以通过 SLOWLOG RESET回收被慢日志消耗的内存slowlog-max-len 1024################################ VM ################################## WARNING! Virtual Memory is deprecated in Redis 2.4### The use ofVirtual Memory is strongly discouraged.# Virtual Memory allows Redis to work with datasets bigger than theactual# amount of RAM needed to hold the whole dataset in memory.# Inorder to do so very used keys are taken in memory while the other keys# areswapped into a swap file, similarly to what operating systems do# withmemory pages.## To enable VM just set &apos;vm-enabled&apos; to yes, and set thefollowing three# VM parameters accordingly to your needs.vm-enabled no# vm-enabled yes# This is the path of the Redis swap file. As you can guess, swapfiles# can&apos;t be shared by different Redis instances, so make sure to use aswap# file for every redis process you are running. Redis will complain ifthe# swap file is already in use.## The best kind of storage for theRedis swap file (that&apos;s accessed at random)# is a Solid State Disk(SSD).## *** WARNING *** if you are using a shared hosting the defaultof putting# the swap file under /tmp is not secure. Create a dir with accessgranted# only to Redis user and configure Redis to create the swap filethere.vm-swap-file /tmp/redis.swap# vm-max-memory configures the VM to use at max the specified amountof# RAM. Everything that deos not fit will be swapped on disk *if* possible,that# is, if there is still enough contiguous space in the swapfile.## With vm-max-memory 0 the system will swap everything it can. Nota good# default, just specify the max amount of RAM you can in bytes, butit&apos;s# better to leave some margin. For instance specify an amount ofRAM# that&apos;s more or less between 60 and 80% of your freeRAM.vm-max-memory 0# Redis swap files is split into pages. An object can be saved usingmultiple# contiguous pages, but pages can&apos;t be shared between differentobjects.# So if your page is too big, small objects swapped out on disk willwaste# a lot of space. If you page is too small, there is less space in theswap# file (assuming you configured the same number of total swap filepages).## If you use a lot of small objects, use a page size of 64 or 32bytes.# If you use a lot of big objects, use a bigger page size.# Ifunsure, use the default :)vm-page-size 32# Number of total memory pages in the swap file.# Given that the pagetable (a bitmap of free/used pages) is taken in memory,# every 8 pages ondisk will consume 1 byte of RAM.## The total swap size is vm-page-size *vm-pages## With the default of 32-bytes memory pages and 134217728 pagesRedis will# use a 4 GB swap file, that will use 16 MB of RAM for the pagetable.## It&apos;s better to use the smallest acceptable value for yourapplication,# but the default is large in order to work in mostconditions.vm-pages 134217728# Max number of VM I/O threads running at the same time.# This threadsare used to read/write data from/to swap file, since they# also encode anddecode objects from disk to memory or the reverse, a bigger# number ofthreads can help with big objects even if they can&apos;t help with# I/O itselfas the physical device may not be able to couple with many# reads/writesoperations at the same time.## The special value of 0 turn off threadedI/O and enables the blocking# Virtual Memoryimplementation.vm-max-threads 4############################### ADVANCED CONFIG################################ 当hash中包含超过指定元素个数并且最大的元素没有超过临界时，#hash将以一种特殊的编码方式（大大减少内存使用）来存储，这里可以设置这两个临界值# RedisHash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，#这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的valueredisObject的encoding为zipmap,#当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。hash-max-zipmap-entries512hash-max-zipmap-value 64# list数据类型多少节点以下会采用去指针的紧凑存储格式。#list数据类型节点值大小小于多少字节会采用紧凑存储格式。list-max-ziplist-entries512list-max-ziplist-value 64# set数据类型内部数据如果全部是数值型，且包含多少节点以下会采用紧凑格式存储。set-max-intset-entries512# zsort数据类型多少节点以下会采用去指针的紧凑存储格式。#zsort数据类型节点值大小小于多少字节会采用紧凑存储格式。zset-max-ziplist-entries128zset-max-ziplist-value 64# Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用##当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。##如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存activerehashing yes################################## INCLUDES#################################### 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件#include /path/to/local.conf# include /path/to/other.conf]]></content>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于并发的一点思考]]></title>
    <url>%2F2018%2F11%2F11%2F%E5%85%B3%E4%BA%8E%E5%B9%B6%E5%8F%91%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[并发编程核心编程模块包括线程安全，线程封闭，线程调度，同步容器，并发容器，AQS，J.U.C等等常用手段有扩容，缓存，队列，应用拆分，限流，服务降级与熔断，数据库切库，分库分表，高可用部署等等并发在了解了基础的方法后本身从理论知识来讲并不难，关键是结合到实际的应用场景时，解决问题的思路和怎样选择最有效的手段，这个需要一定的经验并需要不断的总结和学习。 TODO]]></content>
  </entry>
  <entry>
    <title><![CDATA[Fastdfs分布式文件存储]]></title>
    <url>%2F2018%2F11%2F10%2FFastdfs%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[Fastdfs分布式文件存储 简介&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fastdfs是一款比较轻量级的开源分布式文件系统，作者是余庆大大在0几年业余时间用c撸出来的(工作不饱和，加班不够啊:) ，现任淘宝网开放平台技术部资深架构师,Fastdfs在小文件存储方面性能优异，在淘宝和京东都有实例的应用案例，去哪儿目前使用的是seaweedfs，基于Facebook的图片存储论文用go语言开发的一套存储系统，使用起来也是非常方便，相比Fastdfs更加现代化，这个日后研究，直接进入正题。 相关术语• Tracker Server：跟踪服务器，主要负责任务调度,请求分发负载的工作。记录storage server的状态，是连接Client和Storage server的枢纽。• Storage Server：存储服务器，文件和meta data都保存到存储服务器上。• group：组，也可称为卷。同组内服务器上的文件是完全相同的。• 文件标识：包括两部分：组名和文件名（包含路径）。• meta data：文件相关属性，键值对（Key Value Pair）方式，如：width=1024,heigth=768 系统架构 从图中可以看出两大核心部分，trakcer跟踪器，storage存储节点，跟踪器主要负责路由分发，任务调度，与clent端直接交互，storage则负责存储实际的数据，两部分都支持主流的分布式主从配置，即master/slave模式，在请求量大时可加大trakcer集群规模，在存储大文件或读写频繁时可加大storage集群规模，非常容易的就能做到局部和全局的横向扩展，如果对这两部分理解还很模糊我们看下文件上传下载的流程图 • 1. client询问tracker上传到的storage，不需要附加参数；• 2.tracker返回一台可用的storage；• 3.client直接和storage通讯完成文件上传。 • 1. client询问tracker下载文件的storage，参数为文件标识（组名和文件名）；• 2.tracker返回一台可用的storage；• 3.client直接和storage通讯完成文件下载。 同步机制•同一组内的storage server之间是对等的，文件上传、删除等操作可以在任意一台storage server上进行；•文件同步只在同组内的storage server之间进行，采用push方式，即源服务器同步给目标服务器；•源头数据才需要同步，备份数据不需要再次同步，否则就构成环路了；•上述第二条规则有个例外，就是新增加一台storage server时，由已有的一台storage server将已有的所有数据（包括源头数据和备份数据）同步给该新增服务器，当某台master storage宕机时，依据选举算法此master节点下的某台slave storage会自动切换为master，并将原master剔除集群，原master重启后，会自动以slave的角色重新加入集群，并在重启后将宕机期间的数据自动同步到此节点上。 文件目录结构 运行与安装（单节点）一、准备工作(俩台机器同时进行)1.下载软件: http://sourceforge.net/projects/fastdfs/files/2安装gcc。命令:yum install make cmake gcc gcc-c++ 3 安装libfastcommon(俩台机器同时进行)&nbsp;&nbsp;3.1 上传libfastcommon-master.zip到/usr/local/software下&nbsp;&nbsp;3.2 进行解压libfastcommon-master.zip:命令:unzip libfastcommon-master.zip -d /usr/local/fast/&nbsp;&nbsp;3.3 进入目录:cd /usr/local/fast/libfastcommon-master/ 4 进行编译和安装:命令:&nbsp;&nbsp;./make.sh命令:&nbsp;&nbsp;./make.sh install注意安装的路径:也就是说,我们的libfastcommon默认安装到了/usr/lib64/这个位置。 5 进行软件创建。FastDFS主程序设置的目录为/usr/local/lib/,所以我们需要创建/usr/lib64/下的一些核心执行程序的软连接文件。命令:mk dir /usr/local/lib/命令:ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so命令:ln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.so命令:ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so命令:ln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so6、安装FastDFS&nbsp;&nbsp;6.1 进入到cd /usr/local/software下,解压FastDFS_v5.05.tar.gz文件&nbsp;&nbsp;命令:cd /usr/local/software&nbsp;&nbsp;命令:tar -zxvf FastDFS_v5.05.tar.gz -C /usr/local/fast/&nbsp;&nbsp;6.2安装编译&nbsp;&nbsp;命令:cd /usr/local/fast/FastDFS/&nbsp;&nbsp;编译命令:./make.sh&nbsp;&nbsp;安装命令:./make.sh install7 采用默认安装方式脚本文件说明:1、服务脚本在:/etc/init.d/fdfs_storaged/etc/init.d/fdfs_trackerd2、配置文件在:/etc/fdfs/client.conf.sample/etc/fdfs/storage.conf.sample/etc/fdfs/tracker.conf.sample3、命令行工具在/usr/bin/目录下Fdfs_*的一些列执行脚本4 因为FastDFS服务脚本设置的bin目录为/usr/local/bin/下,但是实际我们安装在了/usr/bin/下面。所以我们需要修改FastDFS配置文件中的路径,也就是需要修改俩个配置文件:命令:vim /etc/init.d/fdfs_storaged进行全局替换命令:%s+/usr/local/bin+/usr/bin命令:vim /etc/init.d/fdfs_trackerd进行全局替换命令:%s+/usr/local/bin+/usr/bin4、配置跟踪器(192.168.1.172节点)1进入cd/etc/fdfs/目录配置跟踪器文件(注意是192.168.1.172节点),把tracker.conf.sample文件进行cope一份:去修改tracker.conf文件 2 修改tracker.conf文件命令:vim /etc/fdfs/tracker.conf如下图所示:我们暂时修改配置文件里的base_path即可。 修改为自己的路径地址:base_path=/fastdfs/tracker注意:对于tracker.conf配置文件参数解释可以找官方文档,地址为:http://bbs.chinaunix.net/thread-1941456-1-1.html3 最后我们一定要创建之前定义好的目录(也就是/fastdfs/tracker):命令:mkdir -p /fastdfs/tracker4 关闭防火墙:(我们在学习时可以不用考虑防火墙的问题)vim /etc/sysconfig/iptables添加:-A INPUT -m state –state NEW -m tcp -p tcp –dport 22122 -j ACCEPT重启:service iptables restart5 启动跟踪器如图所示: 目录命令:cd /fastdfs/tracker/ &amp;&amp; ll启动tracker命令:/etc/init.d/fdfs_trackerd start查看进程命令:ps -el | grep fdfs停止tracker命令:/etc/init.d/fdfs_trackerd stop 6可以设置开机启动跟踪器:(一般生产环境需要开机启动一些服务,如keepalived、linux、tomcat等等)命令:vim /etc/rc.d/rc.local加入配置:/etc/init.d/fdfs_trackerd start 5、配置FastDFS存储(192.168.1.173)1 进入文件目录:cd /etc/fdfs/,进行copy storage文件一份命令:cd /etc/fdfs/命令:cp storage.conf.sample storage.conf2 修改storage.conf文件命令:vim /etc/fdfs/storage.conf修改内容:base_path=/fastdfs/storagestore_path0=/fastdfs/storagetracker_server=192.168.1.172:22122http.server_port=88883 创建存储目录:mkdir -p /fastdfs/storage4 打开防火墙:命令:vim /etc/sysconfig/iptables添加:-A INPUT -m state –state NEW -m tcp -p tcp –dport 23000 -j ACCEPT重启:service iptables restart5 启动存储(storage)命令:/etc/init.d/fdfs_storaged start (关闭:/etc/init.d/fdfs_storaged stop)(初次启动成功后会在/fastdbf/storage/ 目录下创建 data、logs俩个目录)6 查看FastDFS storage 是否启动成功命令:ps -ef | grep fdfs并且我们进入到/fastdfs/storage/data/文件夹下会看到一些目录文件(256*256),如下:命令:cd /fastdfs/storage/data/ &amp;&amp; ls7 同理,也可以设置开机启动存储器:(一般生产环境需要开机启动一些服务,如keepalived、linux、tomcat等等)命令:vim /etc/rc.d/rc.local加入配置:/etc/init.d/fdfs_storaged start到此为止我们的FastDFS环境已经搭建完成。。。 验证环境1 我们先使用命令上传一个文件。注意:是在tracker(跟踪器)中上传。首先我们在跟踪器(192.168.1.172)里copy一份client.conf文件。命令:cd /etc/fdfs/命令:cp client.conf.sample client.conf 2 编辑client.conf文件命令:vim /etc/fdfs/client.conf修改内容:base_path=/fastdfs/trackertracker_server=192.168.1.172:221223 我们找到命令的脚本位置,并且使用命令,进行文件的上传:命令:cd /usr/bin/命令:ls | grep fdfs 4 使用命令fdfs_upload_file进行上传操作:首先,我们先看一下存储器(192.168.1.173),进入到data下,在进入00文件夹下,发现00文件夹下还有一堆文件夹,然后继续进入00文件夹下,最终我们所进入的文件夹为:/fastdfs/storage/data/00/00 里面什么文件都没有。 然后,我们进行上传操作,比如把之前的/usr/local/software/文件夹下的某一个文件上传到FastDFS系统中去,在跟踪器(192.168.1.172)中上传文件,命令如下:命令:/usr/bin/fdfs_upload_file /etc/fdfs/client.conf/usr/local/software/FastDFS_v5.05.tar.gz 最后我们发现,命令执行完毕后,返回一个group1/M00/00/00/…的ID,其实就是返回当前所上传的文件在存储器(192.168.1.173)中的哪一个组、哪一个目录位置,所以我们查看存储器中的/fastdfs/storage/data/00/00文件夹位置,发现已经存在了刚才上传的文件,到此为止,我们的测试上传文件已经OK了。如下 7、FastDFS与Nginx整合1 首先两台机器里必须先安装nginx2然后我们在存储节点上(192.168.1.173)安装fastdfs-nginx-module_v1.16.tar.gz包进行整合。 目录命令:cd /usr/local/software/解压命令:tar -zxvf /usr/local/software/fastdfs-nginx-module_v1.16.tar.gz -C /usr/local/fast/3 进入目录:cd fastdfs-nginx-module/src/ 4 编辑配置文件config命令: vim /usr/local/fast/fastdfs-nginx-module/src/config修改内容:去掉下图中的local文件层次 修改完毕为: 5 FastDFS与nginx进行集成首先把之前的nginx进行删除目录命令:cd /usr/local/删除命令:rm -rf nginx进入到nginx目录命令:cd nginx-1.6.2/加入模块命令:./configure –add-module=/usr/local/fast/fastdfs-nginx-module/src/重新编译命令:make &amp;&amp; make install6 复制fastdfs-ngin-module中的配置文件,到/etc/fdfs目录中,如图所示: copy命令:cp /usr/local/fast/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/7 进行修改 /etc/fdfs/ 目录下,我们刚刚copy过来的mod_fastdfs.conf 文件。 命令:vim /etc/fdfs/mod_fastdfs.conf修改内容:比如连接超时时间、跟踪器路径配置、url的group配置、connect_timeout=10tracker_server=192.168.1.172:22122url_have_group_name = truestore_path0=/fastdfs/storage8 复制FastDFS里的2个文件,到/etc/fdfs目录中,如图所示: 目录命令:cd /usr/local/fast/FastDFS/conf/Copy命令:cp http.conf mime.types /etc/fdfs/9创建一个软连接,在/fastdfs/storage文件存储目录下创建软连接,将其链接到实际存放数据的目录。命令:ln -s /fastdfs/storage/data/ /fastdfs/storage/data/M0010 修改Nginx配置文件,如图所示: 命令:vim nginx.conf修改配置内容如下图所示: 修改内容为:listen 8888;server_name localhost;location ~/group([0-9])/M00 { #alias /fastdfs/storage/data;ngx_fastdfs_module;}注意:nginx里的端口要和第五步配置FastDFS存储中的storage.conf文件配置一致,也就是(http.server_port=8888)11 最后检查防火墙,然后我们启动nginx服务 启动命令:/usr/local/nginx/sbin/nginx,我们刚才上传了一个文件,上传成功,如图: 现在我们使用这个ID用浏览器访问地址:http://192.168.1.173:8888/group1/M00/00/00/wKgBrVaSvM6AddWWAAVFOL7FJU4.tar.gz我们就可以下载这个文件啦!如下图所示: 运维注意:我们在使用FastDFS的时候,需要正常关机,不要使用kill -9强杀FastDFS进程,不然会在文件上传时出现丢数据的情况。到此,我们的FastDFS与Nginx整合完毕!!八:启动停止服务步骤如下:启动命令:启动tracker命令:/etc/init.d/fdfs_trackerd start查看进程命令:ps -el | grep fdfs启动storage命令:/etc/init.d/fdfs_storaged start查看进程命令:ps -el | grep fdfs启动nginx命令:/usr/local/nginx/sbin/nginx停止命令:停止tracker命令:/etc/init.d/fdfs_trackerd stop关闭storage命令:/etc/init.d/fdfs_storaged stop关闭nginx命令:/usr/local/nginx/sbin/nginx -s stop 集群环境安装集群环境安装我就稍微简略点了，99%是相同的，说下差异点，主要是一个group的区分，单节点tracker时没指定group，默认为group0首先准备几台机器192.168.1.113 track-group1192.168.1.114 track-group2192.168.1.115 storage-group1-1192.168.1.116 storage-group1-2192.168.1.117 storage-group2-1192.168.1.118 storage-group2-2 创建tracker节点无区别，依葫芦画瓢创建storage节点时需要指明并区分当前storage节点是挂载在哪个tracker下group的概念主要是为了区分不同的数据来源，可能来自不能的系统，不同的应用，当然在文件传输时不指明group会依照nginx的负载均衡算法(轮询，权重等)分配groupeg: 115 116为group1 117 118为group2vim storage.confgroup_name = group1(或group2)首先一定要启动tracker，之后启动storage，查看日志我们可以发现两个tracker是能互相知道对方的，更能知道当前tracker下挂载了多少个storage，此时storage集群中，拿115 116为例子，一个group中的数据永远是相同的，之间异步进行文件的备份，实现高可用，如果此时115宕机，115会被剔除出group1，恢复后从116进行文件的恢复，并再次加入集群ok,当我们系统中存在多个tracker集群时,此时tracker就不应该直接与客户端通信，在nginx网管层(多层nginx+keeplived虚拟出svip)进行负载均衡 java客户端操作此处结合spring＋mybatis＋springmvc来进行操作，ssm的基本配置不需多说 导入依赖,国产的，包里的代码都有详细的注释，一目了然 12345&lt;dependency&gt; &lt;groupId&gt;com.github.tobato&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client&lt;/artifactId&gt; &lt;version&gt;1.25.4-RELEASE&lt;/version&gt;&lt;/dependency&gt; 添加springmvc中multipartFile的bean配置，此处可以进行文件类型，文件大小，文件编码等设置，不细说，网上一大把 123&lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt;&lt;/bean&gt; 添加fastdfs客户端连接配置，主要是一些连接器，连接池的配置，也可以进行超时时间，签名验签token校验等，这里给出最进本的配置 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--配置扫描包--&gt; &lt;context:component-scan base-package=&quot;com.github.tobato.fastdfs.service,com.github.tobato.fastdfs.domain&quot;/&gt; &lt;!--配置连接管理器--&gt; &lt;bean id=&quot;trackerConnectionManager&quot; class=&quot;com.github.tobato.fastdfs.conn.TrackerConnectionManager&quot;&gt; &lt;constructor-arg name=&quot;pool&quot; ref=&quot;fdfsConnectionPool&quot;&gt; &lt;/constructor-arg&gt; &lt;!--配置fastDFS tracker 服务器 ip:port 地址--&gt; &lt;property name=&quot;trackerList&quot;&gt; &lt;list&gt; &lt;value&gt;10.211.55.5:22122&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!--配置连接池--&gt; &lt;bean id=&quot;fdfsConnectionPool&quot; class=&quot;com.github.tobato.fastdfs.conn.FdfsConnectionPool&quot;&gt; &lt;!--注入连接池配置--&gt; &lt;constructor-arg name=&quot;config&quot; &gt; &lt;bean class=&quot;com.github.tobato.fastdfs.conn.ConnectionPoolConfig&quot;/&gt; &lt;/constructor-arg&gt; &lt;!--注入连接池工厂--&gt; &lt;constructor-arg name=&quot;factory&quot; &gt; &lt;bean class=&quot;com.github.tobato.fastdfs.conn.PooledConnectionFactory&quot;/&gt; &lt;/constructor-arg&gt; &lt;/bean&gt;&lt;/beans&gt; 4.编写test类demo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package org.seckill;import com.github.tobato.fastdfs.domain.StorePath;import com.github.tobato.fastdfs.service.FastFileStorageClient;import org.junit.Test;import org.junit.runner.RunWith;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.AbstractJUnit4SpringContextTests;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import javax.annotation.Resource;import java.io.File;import java.io.FileInputStream;@ContextConfiguration(locations = &#123;&quot;classpath:spring/spring-fastdfs.xml&quot;&#125;)@RunWith(SpringJUnit4ClassRunner.class)public class FastDFSDemo extends AbstractJUnit4SpringContextTests &#123; private static final Logger logger = LoggerFactory.getLogger(FastDFSDemo.class); @Resource private FastFileStorageClient fastFileStorageClient; /** *上传 * * @author ❤ xiemao * @date : 18/11/01 下午8:14 */ @Test public void uploadFile() throws Exception &#123; File file = new File(&quot;/Users/xie199475/备份/provider/src/main/resources/file_source/001.jpg&quot;); StorePath storePath = fastFileStorageClient. uploadFile(null, new FileInputStream(file), file.length(), &quot;jpg&quot;); logger.info(&quot;------:&#123;&#125;&quot;,storePath.getFullPath()); &#125; /** *删除 * * @author ❤ xiemao * @date : 18/11/01 下午8:15 */ @Test public void delete()&#123; String path = &quot;group1/M00/00/00/CtM3BlvUxfGAaC-fAAVFOL7FJU4.tar.gz&quot;; fastFileStorageClient.deleteFile(path); logger.info(&quot;----ok&quot;); &#125; &#125; 12345678910111213141516171819@RequestMapping(value = &quot;/upload&quot;, method = RequestMethod.POST) public void upload(MultipartFile file, FileUpload fileUpload) throws IOException &#123; StorePath storePath = fastFileStorageClient. uploadFile(null, new ByteArrayInputStream(file.getBytes()), Long.valueOf(fileUpload.getSize()), &quot;jpg&quot;); logger.info(&quot;------:&#123;&#125;&quot;, storePath.getFullPath()); &#125; @RequestMapping(value = &quot;/download&quot;, method = RequestMethod.POST) public void download(HttpServletResponse response) throws Exception &#123; try (OutputStream os = response.getOutputStream(); BufferedOutputStream bf = new BufferedOutputStream(os)) &#123; String groupName = &quot;group1&quot;; String path = &quot;M00/00/00/CtM3Blvj9PKAYquVAAD9K7ssbVE777.jpg&quot;; byte[] data = fastFileStorageClient.downloadFile(groupName, path, new DownloadByteArray()); IOUtils.write(data, bf); &#125; &#125; 配置文件参数说明storage.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114storage.conf配置文件分析：#同tracker.confdisabled=false #这个storage服务器属于那个groupgroup_name=group1 #同tracker.confbind_addr= #连接其他服务器时是否绑定地址，bind_addr配置时本参数才有效client_bind=true #同tracker.confport=23000connect_timeout=30network_timeout=60 #主动向tracker发送心跳检测的时间间隔heart_beat_interval=30 #主动向tracker发送磁盘使用率的时间间隔stat_report_interval=60 #同tracker.confbase_path=/opt/fdfsmax_connections=256 #接收/发送数据的buff大小，必须大于8KBbuff_size = 256KB #同tracker.confwork_threads=4 #磁盘IO是否读写分离disk_rw_separated = true #是否直接读写文件，默认关闭disk_rw_direct = false #混合读写时的读写线程数disk_reader_threads = 1disk_writer_threads = 1 #同步文件时如果binlog没有要同步的文件，则延迟多少毫秒后重新读取，0表示不延迟sync_wait_msec=50 #同步完一个文件后间隔多少毫秒同步下一个文件，0表示不休息直接同步sync_interval=0 #表示这段时间内同步文件sync_start_time=00:00sync_end_time=23:59 #同步完多少文件后写mark标记write_mark_file_freq=500 #storage在存储文件时支持多路径，默认只设置一个store_path_count=1 #配置多个store_path路径，从0开始，如果store_path0不存在，则base_path必须存在store_path0=/opt/fdfs#store_path1=/opt/fastdfs2 #subdir_count * subdir_count个目录会在store_path下创建，采用两级存储subdir_count_per_path=256 #设置tracker_servertracker_server=x.x.x.x:22122 #同tracker.conflog_level=inforun_by_group=run_by_user=allow_hosts=* #文件在数据目录下的存放策略，0:轮训 1:随机file_distribute_path_mode=0 #当问及是轮训存放时，一个目录下可存放的文件数目file_distribute_rotate_count=100 #写入多少字节后就开始同步，0表示不同步fsync_after_written_bytes=0 #刷新日志信息到disk的间隔sync_log_buff_interval=10 #同步storage的状态信息到disk的间隔sync_stat_file_interval=300 #线程栈大小thread_stack_size=512KB #设置文件上传服务器的优先级，值越小越高upload_priority=10 #是否检测文件重复存在，1:检测 0:不检测check_file_duplicate=0 #当check_file_duplicate设置为1时，次值必须设置key_namespace=FastDFS #与FastDHT建立连接的方式 0:短连接 1:长连接keep_alive=0 #同tracker.confhttp.disabled=falsehttp.domain_name=http.server_port=8888http.trunk_size=256KBhttp.need_find_content_type=true##include http.conf tracker.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104tracker.conf 配置文件分析：#配置tracker.conf这个配置文件是否生效，因为在启动fastdfs服务端进程时需要指定配置文件，所以需要使次配置文件生效。false是生效，true是屏蔽。disabled=false #程序的监听地址，如果不设定则监听所有地址bind_addr= #tracker监听的端口port=22122 #链接超时设定connect_timeout=30 #tracker在通过网络发送接收数据的超时时间network_timeout=60 #数据和日志的存放地点base_path=/opt/fdfs #服务所支持的最大链接数max_connections=256 #工作线程数一般为cpu个数work_threads=4 #在存储文件时选择group的策略，0:轮训策略 1:指定某一个组 2:负载均衡，选择空闲空间最大的groupstore_lookup=2 #如果上面的store_lookup选择了1，则这里需要指定一个group#store_group=group2 #在group中的哪台storage做主storage，当一个文件上传到主storage后，就由这台机器同步文件到group内的其他storage上，0：轮训策略 1：根据ip地址排序，第一个 2:根据优先级排序，第一个store_server=0 #选择那个storage作为主下载服务器，0:轮训策略 1:主上传storage作为主下载服务器download_server=0 #选择文件上传到storage中的哪个(目录/挂载点),storage可以有多个存放文件的base path 0:轮训策略 2:负载均衡，选择空闲空间最大的store_path=0 #系统预留空间，当一个group中的任何storage的剩余空间小于定义的值，整个group就不能上传文件了reserved_storage_space = 4GB #日志信息级别log_level=info #进程以那个用户/用户组运行，不指定默认是当前用户run_by_group=run_by_user= #允许那些机器连接tracker默认是所有机器allow_hosts=* #设置日志信息刷新到disk的频率，默认10ssync_log_buff_interval = 10 #检测storage服务器的间隔时间，storage定期主动向tracker发送心跳，如果在指定的时间没收到信号，tracker人为storage故障，默认120scheck_active_interval = 120 #线程栈的大小，最小64Kthread_stack_size = 64KB #storage的ip改变后服务端是否自动调整，storage进程重启时才自动调整storage_ip_changed_auto_adjust = true #storage之间同步文件的最大延迟，默认1天storage_sync_file_max_delay = 86400 #同步一个文件所花费的最大时间storage_sync_file_max_time = 300 #是否用一个trunk文件存储多个小文件use_trunk_file = false #最小的solt大小，应该小于4KB，默认256bytesslot_min_size = 256 #最大的solt大小，如果上传的文件小于默认值，则上传文件被放入trunk文件中slot_max_size = 16MB #trunk文件的默认大小，应该大于4Mtrunk_file_size = 64MB #http服务是否生效，默认不生效http.disabled=false #http服务端口http.server_port=8080 #检测storage上http服务的时间间隔，&lt;=0表示不检测http.check_alive_interval=30 #检测storage上http服务时所用请求的类型，tcp只检测是否可以连接，http必须返回200http.check_alive_type=tcp #通过url检测storage http服务状态http.check_alive_uri=/status.html #if need find content type from file extension namehttp.need_find_content_type=true #用include包含进http的其他设置##include http.conf over :)]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>fastdfs</tag>
        <tag>文件存储</tag>
      </tags>
  </entry>
</search>
